\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption,subcaption}
\usepackage{subfig}
\usepackage{enumerate}          % For enumerates indexed by letters
\usepackage{bm}                 % For bold letters
\usepackage{algorithm2e}        % For pseudocode



% 
% Sham's macros
% 
% \input{../macros/gww_defs}
% \input{../macros/gww_chars}
\input{../macros/defs}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass:\ \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Homework 1}
\newcommand{\hmwkDueDate}{April 12, 2016}
\newcommand{\hmwkClass}{CSE 547}
\newcommand{\hmwkAuthorName}{Brian de Silva}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ }\\
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pd}[2]{\frac{\partial}{\partial #1} (#2)}
\newcommand{\pdd}[2]{\frac{\partial #1}{\partial #2}}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\vskip 0.2cm \large Solution:\\}}

% Useful commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand{\bbm}{\begin{bmatrix}}
\newcommand{\ebm}{\end{bmatrix}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\x}{\bm{x}}
\newcommand{\w}{\bm{w}}
\newcommand{\z}{\bm{z}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\norm}[1]{\left\| #1\right\|}


\begin{document}

\maketitle

\pagebreak

\section*{Collaborators} Weston Barger and Emily Dinan on problems 1.1, 1.2, and 1.4.

% Problem 1.1
\begin{homeworkProblem}
    ({\scriptsize Source: KM Exercise 8.6}) {\bf Elementary properties of $l_{2}$ regularized logistic regression}\\
    Consider minimizing

    \begin{flushleft}
    $\hspace{25bp}J(\mathbf{w})=-l(\mathbf{\mathbf{w}},\mathcal{D}_{\textrm{train}})+\lambda\left\Vert \mathbf{w}\right\Vert _{2}^{2}$
    \par\end{flushleft}
    where

    \begin{flushleft}
    $\hspace{25bp}l(\mathbf{\mathbf{w}},\mathcal{D})=\sum_{j}\textrm{ln}\textbf{P}(y^{j}|\mathbf{x}^{j},\mathbf{w})$
    \par\end{flushleft}
    is the log-likelihood on data set $\mathcal{D}$, for $y^{j}\in\{-1,+1\}$.
    Determine whether the following statements are true or false.  Briefly explain.

    \begin{itemize}
        \item[{(a)}] With $\lambda>0$ and the features $x_{k}^{j}$
    linearly separable, $J(\mathbf{w})$ has multiple locally optimal solutions.

        \item[{(b)}] Let $\hat{\mathbf{w}}$= arg min$_{\mathbf{w}}J(\mathbf{w})$ be
    a global optimum. $\hat{\mathbf{w}}$ is typically sparse (has many zero entries).

        \item[{(c)}] If the training data is linearly separable, then some weights $w_{j}$
    might become infinite if $\lambda=$0.
        
        \item[{(d)}] $l(\mathbf{\mathbf{\hat{w}}},\mathcal{D}_{\textrm{train}})$ always increases
    as we increase $\lambda$.

        \item[{(e)}] $l(\mathbf{\mathbf{\hat{w}}},\mathcal{D}_{\textrm{test}})$ always increases
    as we increase $\lambda$.
    \end{itemize}

    \textbf{Now answer the following questions:}
    \begin{itemize}
      \item[{(f)}] Can the decision boundary in unregularized logistic regression
      change if we add an additional variable that is a duplicate of one of the
      variables already in the model?
      Give an intuitive answer (Hint: think about the model assumptions).
      \item[{(g)}] Let us say our $\mathcal{D}_{\textrm{train}}$ has $n$ examples,
      and only one feature $x_1$. Now we create a dataset
      $\mathcal{D}_{\textrm{mod}}$, with $n$ examples and two features $x_1$ and
      $x_2$ where each example in $\mathcal{D}_{\textrm{mod}}$ contains the feature
      on $\mathcal{D}_{\textrm{train}}$ twice, and the same label. We learn a
      logistic regression from $\mathcal{D}_{\textrm{train}}$, which will give us
      two parameters: $w_0$ and $w_1$. We also learn a logistic regression from
      $\mathcal{D}_{\textrm{mod}}$, which will give us three paramters: $w_0'$,
      $w_1'$, $w_2'$.

      \begin{itemize}
      \item[i.] Write down the \textbf{unregularized} log-likelihood we want to maximize for each of the
      two logistic regressions.
      \item[ii.] Given the log-likelihood functions, what is the relationship
      between $(w_0, w_1)$ and $(w_0', w_1', w_2')$? Using this relationship, answer
      question (f) again here, more formally.
      \item[iii.] Would your answer for the previous question change if we were
      using $l_2$ regularization?. Argue why or why not (Remember we don't regularize $w_0$).
      \end{itemize}
    \end{itemize}

    \solution
    \begin{enumerate}[(a)]
        \item False. With $\lambda>0$ and $x_k^j$ linearly separable, both $-l(\bm{w},\mathcal{D})$ and $\lambda\|\bm{w}\|_2^2$ are convex. In fact $\lambda\|\bm{w}\|_2^2$ is strictly convex. Hence $J(\bm{w})$ is convex as well since the sum of convex functions is convex. Since $J(\bm{w})$ is convex. It is easy to show that the strict convexity of $\lambda\|\bm{w}\|_2^2$ implies that $J(\bm{w})$ is also strictly convex. Therefore $J(\bm{w})$ has at most one minimum.
        \item False. Notice that without the 2-norm penalty term it is very unlikely that $\hat \w$ would be sparse as any weight $\hat w_i$ being zero means that the $i$th feature can be safely ignored. Only in very special instances will many such features exist. For the 2-norm regularization term $\lambda \|\w\|_2^2=\lambda\sum_i w_i^2$, weights $w_i$ that are small, but nonzero do not make much of a contribution since they become smaller when squared. In order for the model to prioritize shrinking the size of these weights in optimizing $J(\w)$, $l(\w,\mathcal{D}_{\text{train}})$ must be $\mathcal{O}(\lambda w_i^2)$ as well. However, $l(\w,\mathcal{D})$ is bounded above by 0 and in order for it to actually approach 0, $\|\w\|$ must become large (since we assume $\bm{P}(y|\x,\w))$ is a sigmoid function which never reaches 0 or 1, but can approach them for large $\|\w\|$). But if $\|\w\|$ is large, then there is some $k$ such that $|w_k|$ is large, making $J(\w)$ large. Thus it is very unlikely that the global minimizer for $J(\w)$ has many zero entries.
        \item True. Assume for simplicity that we are not using an offset $w_0$. If the training data is linearly separable then we can find some weight vector $\w$ such that level set $\{\z|\w^T\z=0\}$ separates the points $\x^j$ corresponding to $y^j=-1$ from the points $\x^j$ corresponding to $y^j=1$. Note that this also entails that $\w^T\x^j\neq0$ for each $j$. Then we can make the following observations:
        \begin{enumerate}
          \item $\alpha \w$ also separates the two sets of points for any $\alpha\neq 0$ since $\w^T\z=0$ if and only if $(\alpha\w)^T\z=\alpha(\w^T\z)=0$.
          \item If $\alpha>1$ then $l(\alpha\w,\mathcal{D})>l(\w,\mathcal{D})$. This follows from our definition of $\bm{P}(y^j,\x^j,\w)$:
          \begin{align*}
            \bm{P}(y^j=1|\x^j,\w)&=\frac{1}{1+\exp(-\w^T\x)}\\
            \bm{P}(y^j=-1|\x^j,\w)&=\frac{1}{1+\exp(\w^T\x)}.
          \end{align*}
          This definition implies that
          \[
            l(\alpha\w,\mathcal{D})=-\sum_j\log(1+\exp(-\alpha  y^j\w^T\x)).
          \]
          We can see from here that $l(\alpha\w,\mathcal{D})>l(\w,\mathcal{D})$ if $\alpha>1$.
        \end{enumerate}
          From these observations we see that to minimize $-l(\alpha\w,\mathcal{D}_{\text{train}})$ (hence $J(\w)$) we should let $\alpha\to\infty$. But if any entry of $\w$ is nonzero then this optimal solution will have entries that go to infinity as $\alpha\to\infty$.
        \item False. Minimizing $J(\w)$ is a tradeoff between simultaneously maximizing $l(\w,\mathcal{D}_{\text{train}})$ and minimizing $\lambda\|\w\|_2^2$. As we saw before, if $\lambda=0$ (and the data are linearly separable) then to get $l(\w,\mathcal{D}_{\text{train}})$ to approach its maxima of 0, we often need $\hat \w$ to have infinitely large entries. As we increase $\lambda$ from 0 we are increasingly restricted in how large the entries of $\hat \w$ can be (clearly the minimum is not attained by a $\hat \w$ with infinitely large entries). This imposes increasingly severe restrictions on how well we are able to maximize $l(\w,\mathcal{D}_{\text{train}})$, meaning that $l(\hat\w,\mathcal{D}_{\text{train}})$ will often {\it decrease} as $\lambda$ is increased.
        \item False. Consider the case where $\mathcal{D}_{\text{test}}=\mathcal{D}_{\text{train}}$. Then by the same reasoning as in the previous part, $l(\hat \w,\mathcal{D}_{\text{test}})$ does not necessarily increase as we increase $\lambda$.

        \item No. If the additional variable is simply a copy of an existing variable then the model is not actually afforded any extra flexibility in separating the points.
        % Yes. In using logistic regression we assume that the variables are independent. Two variables that are duplicates of one another are clearly dependent, violating this assumption.

        \item 
        \begin{itemize}
          \item[i.] For the first logistic regression the log-likelihood we want to maximize is
          \[
            l(\w,\mathcal{D}_{\text{train}})=-\sum^n_{j=1}\log(1+\exp(-y^j(w_0+w_1x_1^j))).
          \]
          For the second it is
          \[
            l(\w,\mathcal{D}_{\text{train}})=-\sum^n_{j=1}\log(1+\exp(-y^j(w_0'+w_1'x_1^j+w_2'x_2^j))) = -\sum^n_{j=1}\log(1+\exp(-y^j(w_0'+(w_1'+w_2')x_1^j)))
          \]
          as $x_1^j=x_2^j$.
          \item[ii.] We would like to choose the weights that maximize these two log-likelihoods. The expression for the second log-likelihood function is identical to the first, except $w_1$ has been replaced with $w_1'+w_2'$. It follows that if we find weights that maximize the first log-likelihood, $\hat w_0$ and $\hat w_1$, then we can maximize the second by setting $w_0'=\hat w_0$ and $w_1'+w_2'=\hat w_1$.

          In a more general setting the $d+1$ weights $w_0,w_1,w_2,\dots,w_d$ are to be learned by maximizing
          \[
            l(\w,\mathcal{D}_{\text{train}})=-\sum^n_{j=1}\log(1+\exp(w_0+\w^Tx^j)).
          \]
          Suppose we find such weights which maximize the log-likelihood:, $\hat w_0,\hat \w$. If we consider a similar problem on $\mathcal{D}_{\text{mod}}$ we will attempt to maximize
          \begin{align*}
            l(\w,\mathcal{D}_{\text{mod}})&=-\sum^n_{j=1}\log(1+\exp(w_0+w_1x_1^j+\dots+w_dx_d^j+w_{d+1}x_{d+1}^j))\\
            &=-\sum^n_{j=1}\log(1+\exp(w_0+(w_1+w_{d+1})x_1^j+\dots+w_dx_d^j)).
          \end{align*}
          It is clear that the optimal weights for this case are simply $w_j=\hat w_j$ for $j=0,2,3,\dots,d$ and $w_1+w_{d+1} = \hat w_1$. The weight multiplying the first feature has simply been reparameterized by two variables instead of one, but no extra degrees of freedom have actually been added. Since the second model is constrained to pick its weights in such a manner, the decision boundary will not actually change when the extra variable is added (all points will be classified the same way they were before).

          % (assume for simplicity that we do not use an offset, $w_0$), our decision boundary is the set $\{\z|\w^T\z=0\}$, i.e. the components of any vector on the decision boundary satisfy $z_1w_1+z_2w_2+\dots+z_nw_n=0$. If we introduce an additional variable that is a duplicate of another, say the first, then the new inner product we compute to decide how to classify points is 
          % \[
          %   \tilde w_1 z_1+\tilde w_2z_2+\dots+\tilde w_nz_n+\tilde w_{n+1}z_{n+1}=(\tilde w_1+\tilde w_{n+1}) z_1+\tilde w_2z_2+\dots+\tilde w_nz_n.
          % \]

          % In order for the decision boundary to stay the same we would need $\tilde w_1+\tilde w_{n+1}=w_1$, showing that***
          \item[iii.] If we use $L^2$ regularization then in the first case we would need to minimize
          \[
            J(\w)=-l(\w,D_{\text{train}})+\lambda\|\w\|_2^2=l(\w,D_{\text{train}})+\lambda w_1^2.
          \]
          Suppose this is minimized by $\w=(w_0,w_1)$. Then for the second case we would need to minimize
          \[
            J(\w)=-l(\w,D_{\text{train}})+\lambda\|\w\|_2^2=l(\w,D_{\text{train}})+\lambda((w_1')^2+(w_2')^2).
          \]
          In the previous part we found that we could take $w_0'=w_0$ and $w_1'+w_2'=w_1$ to find the minimizer. This will make the value of the two log-likelihoods the same, but the $L^2$ terms will not match, as 
          \[
            w_1^2=(w_1'+w_2')^2=(w_1')^2+2w_1'w_2'+(w_2')^2\neq (w_1')^2+(w_2')^2.
          \]
          Hence the relation from the previous part will not necessarily give the minimizer for the second logistic regression model.
        \end{itemize}
    \end{enumerate}

\end{homeworkProblem}


% Problem 1.2
\begin{homeworkProblem}
  On Slide 7 of the first lecture, we presented multi-class logistic regression,
  where $Y \in \{y_1, ..., y_R\}$. Here, we have a simplified version, with no
  $w_0$. When $k < R$, the posterior probability is given by:
  \begin{equation*}
  P(Y = y_k | X) = \frac{\exp(\langle w_{k}, X \rangle)}{1 + \sum_{j =
  1}^{R-1}\exp(\langle w_{j}, X \rangle)}
  \end{equation*}
  For $k = R$, the posterior is:
  \begin{equation*}
  P(Y = y_k | X) = \frac{1}{1 + \sum_{j = 1}^{R-1}\exp(\langle w_j , X \rangle)}
  \end{equation*}
  Where $\langle w_j, X \rangle = \sum_{i=1}^{n}w_{ji}X_i$ (i.e. the dot product).
  We can replace the two equations above by a single equation, to simplify
  notation. For such, we introduce a fixed, pseudo parameter vector $w_R =
  [0,0,0,...,0]$. Now, for any label $y_k$, we write:
  \begin{equation*}
  P(Y = y_k | X) = \frac{\exp(\langle w_{k}, X \rangle)}{1 + \sum_{j =
  1}^{R-1}\exp(\langle w_{j}, X \rangle)}
  \end{equation*}
  \begin{enumerate}[(a)]
    \item[(a)] How many parameters do we need to estimate? What are these parameters?
    \item[(b)] Given $N$ training samples $\{(x^1, y^1), (x^2, y^2), ..., (x^N,y^N)\}$,
    write down explicitly the log-likelihood function and simplify it as much as you
    can:
    \begin{equation*}
    L(w_1, ..., w_{R-1}) = \sum_{j = 1}^{N}ln(P(y^j| x^j,w))
    \end{equation*}
    \item[(c)] Compute the gradient of $L$ with respect to each $w_k$ and simplify
    it.
    \item[(d)] Now add the regularization term $\dfrac{\lambda}{2}$ and define a new
    objective function:
    \begin{equation*}
    L(w_1, ..., w_{R-1}) = \sum_{j = 1}^{N}ln(P(y^j| x^j,w)) - \frac{\lambda}{2}\sum_{l=1}^{R-1}\norm{w_l}_2^2
    \end{equation*}
    Compute the gradient of this new $L$ with respect to each $w_k$.
  \end{enumerate}

  \solution
  \begin{enumerate}[(a)]
    \item The parameters we need to estimate are the entries of each of the vectors $w_1,w_2,\dots,w_{R-1}$. Since each of these has $n$ entries, we must estimate $(R-1)n$ parameters total.
    \item For this problem we use the convention introduced above that $w_R=[0,0,\dots,0]$. Furthermore, let $I(y_k)=k$ so that if $y^j=y_k$ then $I(y^j)=k$. Then
    \begin{align*}
      L(w_1,\dots,w_{R-1})&=\sum_{j=1}^N\log(\bm{P}(y^j|x^j,w))\\
      &= \sum_{j=1}^N\log\left(\frac{\exp(\langle w_{I(y^j)}, x^j \rangle)}{1 + \sum_{i=1}^{R-1}\exp(\langle w_{i}, x^j \rangle)} \right)\\
      &= \sum_{j=1}^N\left(\log(\exp(\langle w_{I(y^j)},x^j\rangle))-\log\left(1+\sum^{R-1}_{i=1}\exp(\langle w_i,x^j\rangle)\right)\right)\\
      &=\sum^N_{j=1}\left( \langle w_{I(y^j)},x^j\rangle -\log\left(\sum^{R}_{i=1}\exp(\langle w_i,x^j\rangle) \right)\right).
    \end{align*}
    \item Let $\bm{1}$ denote the identity function defined as
    \[
      \bm{1}(y=k)=\begin{cases}
        1 &\text{if~}y=k\\
        0 & \text{if~}y\neq k.
      \end{cases}
    \]
    Suppose $k\neq R$. Then the gradient of one log likelihood term with respect to $w_k$ is
    \begin{align*}
      \frac{\partial }{\partial w_k}\log(\bm{P}(y^j|x^j,w)) &= \left(\frac{\partial }{\partial w_k}\langle w_{I(y^j)},x^j\rangle - \frac{\partial }{\partial w_k}\log\left(\sum^{R}_{i=1}\exp(\langle w_i,x^j\rangle) \right)\right)\\
      &= \bm{1}(y^j=k)x^j - \frac{x^j\exp(\langle w_k,x^j\rangle)}{\sum^R_{i=1}\langle w_i,x^j\rangle} \\
      &= x^j\left(\bm{1}(y^j=k) -\frac{\exp(\langle w_k,x^j\rangle)}{\sum^R_{i=1}\langle w_i,x^j\rangle}  \right).
    \end{align*}
    We can also write this gradient as
    \[
       \frac{\partial }{\partial w_k}\log(\bm{P}(y^j|x^j,w)) = \begin{cases}
         x^j(\bm{1}(y^j=k) -\bm{P}(y^j=k|x^j,w))\\
         % \frac{\exp(\langle w_k,x^j\rangle)}{\sum^R_i=1}\langle w_i,x^j\rangle}  \right) & \text{if~} y^j\neq R\\
         x^j\bm{1}(y^j=R) & \text{if~} y^j=R
       \end{cases}.
    \]
    If $k=R$ then the gradient is the zero vector since $w_R$ is a pseudo parameter vector which is fixed. Now we can easily write down the gradient of $L(w)$ with respect to $w_k$ (for $k\neq R$):
    \begin{align*}
      \frac{\partial }{\partial w_k}L(w_1,w_2,\dots,w_{k-1}) &= \frac{\partial }{\partial w_k}\sum_{j=1}^N\log(\bm{P}(y^j|x^j,w))\\
      &=\sum_{j=1}^N \frac{\partial }{\partial w_k} \log(\bm{P}(y^j|x^j,w))\\
      &=\sum_{j=1}^N x^j\left[\bm{1}(y^j=k) -\bm{P}(y^j|x^j,w)  \right].
    \end{align*}
    \item This new objective function is the same as the previous one except it includes an $L^2$ regularization term. Taking the gradient of this term with respect to $w_k$ (for $k\neq R$) gives
    \[
      -\frac{\partial }{\partial w_k}\frac\lambda2\sum^{R-1}_{l=1}\norm{w_l}_2^2 = -\lambda w_k.
    \]
    Hence the gradient of $L$ with respect to $w_k$ is
    \[
      \frac{\partial }{\partial w_k}L(w_1,w_2,\dots,w_{k-1}) = x^j\left[\bm{1}(y^j=k) -\bm{P}(y^j|x^j,w)  \right] - \lambda w_k.
    \]
  \end{enumerate}
\end{homeworkProblem}


% Problem 1.3
\begin{homeworkProblem}
    The Count-Min sketch of Cormode and Muthukrishnan is biased.  That is, the estimated count $\hat{a}_i$ for element $i \in \{1,\ldots,N\}$ is always higher than (or equal to) the true count $a_i$. Reminder: The count $a_i$ is the number of times we see element $i$ in the sequence.   In this question, you will develop a simple unbiased sketch, \emph{Simple-Count}, (with weaker convergence rates than the Count-Min sketch). First, we will start with the simplest version of Simple-Count:  Let $g$ be a hash function chosen from a family $G$ of independent hashes, such that $g$ maps each $i$ to either $+1$ or $-1$ with equal probability \footnote{The randomness arises from the fact that the hash function $g$ is drawn randomly from the family $G$.  Given a hash function $g$, the mapping $g: \{1,\dots,N\}$ is deterministic.  All expectations, etc. are taken with respect to the distribution of $g$.}:
  \[
  P(g(i) = +1) = P(g(i) = -1) = 1/2. 
  \]
  We now define $h$, the accumulator of our sketch.  When we observe element $i$ in the
  sequence, we simply update:
  \[
  h =h + g(i).
  \]
  Now, if we would like to predict the count for element $i$,
  we simply return:
  \[
  \hat{a}_i = h \  g(i).
  \]
  Given this sketch, please answer the following questions:

  \begin{itemize}
  \item[{(a)}] Let $a_i$ be the true counts for each element $i$.  Express
    $h$ in terms of the $a_i$ and $g(i)$ only.
  \item[{(b)}] What is the expected value of $g(i)$, denoted by $E[g(i)]$?
  \item[{(c)}] Prove that $\hat{a}_i = h \  g(i)$ is an unbiased estimate
  of $a_i$, i.e., $E[\hat{a}_i] = a_i$.  Hint: use linearity of
  expectations, $E[u+v] = E[u] + E[v]$, and the fact that $g(i)$ and
  $g(j)$ are independent.  
  \item[{(d)}] Prove that the variance of our estimate $Var(\hat{a}_i)$ is
    given by:
  \[
  Var(\hat{a}_i) = \sum_{j\in\{1,\ldots,N\}:j\neq i} a_j^2.
  \]
  Hint: recall that $Var(X) = E[X^2] - (E[X])^2$.
  \item[{(e)}] We will now bound the probability of getting a bad
    estimate.  In particular, after $n$ steps, we will say our estimate
    $\hat{a}_i$ is $\epsilon$-bad if, for $\epsilon>0$:
  \[
  |\hat{a}_i - a_i| \geq \epsilon n.
  \]
  To prove our bound, we will use Chebyshev's inequality: If $X$ is a
  random variable, and $\alpha >0$, then:
  \[
  P(|X-E[X]| \geq \alpha) \leq \frac{Var(X)}{\alpha^2}.
  \]
  Use Chebyshev's inequality to prove that the probability
    $\delta$ of getting a bad estimate for $\hat{a}_i$ is bounded by:
  \[
  \delta \leq \frac{Var(\hat{a}_i)}{\epsilon^2 n^2} \leq \frac{1}{\epsilon^2} .
  \] 
  \item[{(f)}] The bound in the previous question is going to be vacuous for
    sufficiently small $\epsilon$.  To address this issue, we will
    expand the number of hash functions in our sketch.  Let's introduce
    a set of $k$ independent hash functions $g_j$ with the same properties
    as $g$ above.  Now, we will create $h_j$, in analogy to the $h$ function
    above, for each $g_j$.  When we see element $i$ in the sequence, we
    will update each $h_j$ by:
  \[
  h_j = h_j + g_j(i).
  \]
  Now, if we would like to predict the count for element $i$,
  we simply return the average:
  \[
  \hat{a}_i = \frac{1}{k} \sum_{j=1}^k h_j \  g_j(i).
  \]
  For this sketch, prove that:
  \begin{enumerate}
  \item[{i.}] The variance of $\hat{a}_i$ is now bounded by:
  \[
  Var(\hat{a}_i) \leq \frac{n^2}{k}.
  \]
  \emph{Hint: The estimates obtained by each hash function are independent.} 
  \item[{ii.}] Use this result and the Chebyshev's inequality as above to
    prove that for any $\epsilon>0,\delta>0$, the probability of getting an $\epsilon$-bad estimate of
    $\hat{a}_i$ will be lower than $\delta$ if we use $k \geq
    \frac{1}{\delta\epsilon^2}$ hash functions.
  \end{enumerate}
  \end{itemize}

  \solution
  \begin{enumerate}[(a)]
    \item For each $i$, $g(i)$ is added to the accumulator $a_i$ times, so we can express $h$ as
    \[
      h=\sum^N_{i=1}a_ig(i).
    \]
    \item Taking the expectation with respect to the family of hash functions, $G$, we have
    \[
      E[g(i)]=1\cdot \bm{P}(g(i)=1) - 1\cdot \bm{P}(g(i)=-1) = \frac12 - \frac12 = 0.
    \]
    \item Recall that if $X$ and $Y$ are independent, $E[XY]=E[X]E[Y]$. We have
    \begin{align*}
      E[\hat a_i]&= E[hg(i)]\\
      &=E\left[\left(\sum_{j=1}^Na_jg(j)\right)g(i)\right]\\
      &=E\left[\left(\sum_{j=1}^Na_jg(j)g(i)\right)\right]\\
      &=\sum_{j=1}^N a_j E\left[g(j)g(i)\right].
    \end{align*}
    Since $g(i)$ and $g(j)$ are independent if $i\neq j$ and $E[g(j)]=0$, the expression above becomes
    \begin{align*}
      E[\hat a_i]&=a_iE[g^2(i)] + \sum_{j=1:j\neq i}^Na_jE[g(j)]E[g(i)] \\
      &= a_i E[g^2(i)]\\
      &= a_i.
    \end{align*}
    We have also used that $g^2\equiv 1$. Thus $\hat a_i$ is an unbiased estimate of $a_i$.
    \item First note that 
    \[
      Var(\hat a_i) = E[(\hat a_i)^2] - (E[\hat a_i])^2 = E[(\hat a_i)^2]-a_i^2
    \]
    by the previous part. Therefore we just need to compute $E[(\hat a_i)^2]$. To this end
    \begin{align*}
      E[\hat a_i^2]&= E[(hg(i))^2]\\
      &= E[h^2]\\
      &= E\left[\left(\sum_{j=1}^Na_jg(j)\right)^2 \right]\\
      &= E\left[\sum_{j=1}^Na_j^2 +2\sum_{j,k=1:j\neq k}^Na_ja_kg(j)g(k)\right]\\
      &= \sum_{j=1}^NE[a_j^2] + 2\sum\sum_{j,k=1:j\neq k}a_ja_kE[g(j)g(k)]\\
      &= \sum_{j=1}^N a_j^2.
    \end{align*}
    Note that we have again used the our result from before that since $g(i)$ and $g(j)$ are independent if $i\neq j$ and $E[g(j)]=0$, $E[g(i)g(j)]=E[g(i)]E[g(j)]=0$ if $i\neq j$. Thus
    \[
      Var(\hat a_i) = \sum_{j=1}^N a_j^2 - a_i^2 = \sum_{j=1:j\neq i}^N a_j^2.
    \]
    \item We claim that $\sum_{j=1:j\neq i}^N a_j^2 \leq n^2$. To see this note that 
    \[
      n = \sum^N_{j=1}a_j
    \]
    since $n$ is the total number of elements we have witnessed in the sequence. Squaring both sides we obtain
    \[
      n^2 = \left( \sum^N_{j=1}a_j\right)^2 = \sum^N_{j=1}a_j^2 + 2\sum^N_{j=1:j\neq i}a_ia_j\geq \sum^N_{j=1:j\neq i}a_j^2
    \]
    for any given $i\in\{1,2,\dots,N\}$ (recall that each $a_j\geq 0$, so all the terms in the above are nonnegative). This establishes the claim, which in turn implies that
    \[
      \frac{\sum_{j=1:j\neq i}^N a_j^2}{n^2}\leq 1.
    \]
    Now, by Chebyshev's inequality we have
    \begin{align*}
      \bm{P}(|\hat a_i-E[\hat a_i]|\geq \epsilon n) &= \bm{P}(|\hat a_i-a_i\geq \epsilon n)\\
      &\leq \frac{Var(\hat a_i)}{(\epsilon n)^2}\\
      &=\frac{\sum^N_{j=1:j\neq i}a_j^2}{\epsilon^2 n^2}\\
      &\leq \frac{1}{\epsilon^2}.
    \end{align*}
    \item
    \begin{itemize}
      \item [i.] Since the estimates produced by each hash function are independent, so are the terms in the sum, $h_jg_j(i)$. Recall that if two random variables $X$ and $Y$ are uncorrelated, then $Var(X+Y)=Var(X)+Var(Y)$. Therefore, using the claim from the previous question,
      \begin{align*}
        Var(\hat a_i) &= Var\left(\frac1k\sum^k_{j=1}h_jg_j(i) \right)\\
        &= \frac{1}{k^2}Var\left(\sum^k_{j=1}h_jg_j(i) \right)\\
        &= \frac{1}{k^2}\sum^k_{j=1}Var(h_jg_j(i))\\
        &= \frac{1}{k^2}\sum^k_{j=1}\sum^N_{l=1:l\neq i}a^2_l\\
        &\leq\frac{1}{k^2}\sum^k_{j=1}n^2\\
        &= \frac{1}{k^2}kn^2\\
        &= \frac{n^2}{k}.
      \end{align*}
      This shows that the probability $\delta$ of getting an $\epsilon$-bad estimate for $\hat a_i$ is bounded above by $\tfrac{1}{\epsilon^2}$.

      \item [ii.] Observe that because each of the hash functions $g_j$ satisfy the same properties as $g$ above, so too do the $h_j$'s  satisfy the same properties as $h$ above. Therefore
      \begin{align*}
        E[\hat a_i]&=E\left[ \frac1k\sum^k_{j=1}h_jg_j(i)\right]\\
        &= \frac1k\sum^k_{j=1}E[h_jg_j(i)]\\
        &= \frac1k\sum^k_{j=1}a_i\\
        &= a_i.
      \end{align*}
      Chebyshev's inequality then allows us to bound the probability that we obtain an $\epsilon$-bad estimate of $\hat a_i$ as follows
      \begin{align*}
        \bm{P}(|\hat a_i-E[\hat a_i]\geq \epsilon n) &= \bm{P}(|\hat a_i-a_i\geq \epsilon n)\\
        &\leq \frac{Var(\hat a_i)}{(\epsilon n)^2}\\
        &\leq \frac{n^2}{k}\frac{1}{\epsilon^2 n^2}\\
        &=\frac{1}{k\epsilon^2}.
      \end{align*}
      This says that if $k$ is chosen such that $k\geq \tfrac{1}{\delta \epsilon^2}$, then the probability of getting an $\epsilon$-bad estimate of $\hat a_i$ will be bounded by
      \[
        \frac{1}{k\epsilon^2}\leq \frac{\epsilon^2 \delta}{\epsilon^2}=\delta.
      \]
    \end{itemize}
  \end{enumerate}

\end{homeworkProblem}

% Problem 1.4
\begin{homeworkProblem}

  {\bf 1. Warm up}\\
  We begin by simply assessing various attributes of the dataset, primarily to ensure that it is correctly accessed and parsed.
  \\
  %If you are using the starter code, please complete the functions in``analysis/BasicAnalysis.py''.
  \\ In the starter code, you will find ``analysis/DummyLoader.py'' as sample code for initializing the dataset, iterating over each row, parsing the text, and printing out results.  \textbf{NOTE}: You will have to change the location of the dataset in \texttt{main} to reflect where you put the data on your system.

  \begin{enumerate}[(a)]
    \item Report the average CTR for the training data (Number of clicks / Number of examples).
    \item How many unique tokens are there in the training data? What about the test data? How many tokens appear in both datasets?
  %  \item How many unique users are there in the training data? What about the test data? How many users appear in both datasets?
    \item How many unique users are there in each age group in the training data? What about the test data?
  \end{enumerate}

  \solution
  \begin{enumerate}[(a)]
    \item I found the average CTR for the training data to be 0.033655.
    \item I found that the number of unique tokens in the training set was 141063, the number in the testing set was 109459, and the number in both sets was 79261.
    \item I found that the age groups of the training and testing sets had the following numbers of unique users:
    \begin{centering}
      \begin{tabular}{c|c|c}
        {\bf Age group}& {\bf Unique users (training)} & {\bf Unique users (testing)}\\ \hline
        0 &8826   &2021   \\
        1 &79668  &48783  \\
        2 &162725 &113818 \\
        3 &307482 &178416 \\
        4 &213292 &117326 \\
        5 &146605 &79635  \\
        6 &63834  &34908  \\
      \end{tabular}
    \end{centering}
  \end{enumerate}

  \vskip 1cm
  {\bf 2. Stochastic Gradient Descent}
  \\
  \\Recall that stochastic gradient descent (SGD) performs a gradient descent using a noisy estimate of the full gradient based on just the current example.

  \begin{enumerate}[(a)]
    \item Write down the equation for the weight update step. That is, how to update weights $w^t$ using the data point $(x^t, y^t)$, where $x^t \equiv [x_1^t, x_2^t, \dots, x_d^t]$ is the feature vector for example $t$, and $y^t \in \{0, 1\}$ is the label. 

    \item  For stepsizes $\eta = \{0.001, 0.01, 0.05\}$ and without regularization, implement SGD and train the weights by making one pass over the dataset. \textbf{Use only one pass over the data on all subsequent questions as well.}  For each step size:
      \begin{itemize}
        \item  Plot the average loss $\overline{L}$ as a function of the number of steps $T$, where
          \[
          \overline{L}(T) = \frac{1}{T} \sum_{t=1}^T (\hat{y}^t-y^t)^2
          \]
          where $\hat{y^t}$ is the predicted label of example $x^t$ using the weights $w^{t-1}$.
          Record the average loss every 100 steps, e.g. $[100, 200, 300, \dots]$.

        \item Report the $l_2$ norm of the weights at the end of the pass.
        \item Use the weights to predict the CTRs for the test data. Recall that ``test\_label.txt'' contains the labels for the test data.
               Report the RMSE (root mean square error) of your predicted CTR.   
               Also report the RMSE of the baseline prediction you got from the Warm Up.  (Do not expect a huge improvement since the label distribution is biased. Elgoog still makes a huge profit even with a $0.1\%$ improvement in accuracy.)
               \\
              %\emph{Hint: you can use the given Util/EvalUtil.py to compute RMSE}.
              \emph{Hint: you can use the given Util/EvalUtil.py to compute RMSE}.
      \end{itemize}
      \item For $\eta = 0.01$, report the weights for the following features: intercept, ``Position'', ``Depth'', ``Gender'', and ``Age''. Provide an interpretation of the effect of each feature on the probability of a click based on these inferred weights.
  \end{enumerate}

  \solution

  \begin{enumerate}[(a)]
    \item Since $y^t\in\{0,1\}$, the log-likelihood for a single point $(x^t,y^t)$ is
    \begin{align*}
      l(\w) &= y^t \log(\bm{P}(y^t=1|x^t,\w)) + (1-y^t)\log(\bm{P}((y^t=0|x^t,\w)))\\
      &= y^t \log\left(\frac{\exp(w_0 + \langle \w,x^t\rangle)}{1+\exp(w_0 + \langle \w,x^t\rangle)}\right) + (1-y^t)\log\left(\frac{1}{1+\exp(w_0 + \langle \w,x^t\rangle)}\right)\\
      &=y^t(w_0 + \langle \w,x^t\rangle) - \log(1+\exp(w_0 + \langle \w,x^t\rangle)).
    \end{align*}
    The gradient of this log-likelihood with respect to $\w$ (ignoring $w_0$ for the moment) is then
    \begin{align*}
      \frac{\partial l}{\partial \w}&=x^ty^t - \frac{x^t \exp(w_0 + \langle \w,x^t)}{1+\exp(w_0 + \langle \w,x^t\rangle)} \\
      &= x^t\left(y^t - \bm{P}(y^t=1|x^t,\w) \right).
    \end{align*}
    Hence the SGD update using one data point $(x^t,y^t)$ is
    \[
      \w^{t+1} = \w^t - \eta x^t\left(y^t - \bm{P}(y^t=1|x^t,\w) \right)
    \]
    For $w_0$ the SGD update changes slightly as there are no $w_0x^t$ terms:
    \[
      w_0^{t+1} = w_0^t - \eta \left(y^t - \bm{P}(y^t=1|x^t,\w) \right).
    \]
    \item 
    \begin{figure}
      \centering
      \includegraphics[width=.75\textwidth]{figures/error_2b}
      \caption{Average loss training logistic regression to predict CTR using vanilla unregularized SGD} 
      \label{fig:avg_loss_SGD}
    \end{figure}

    \begin{table}[h]
    \centering
      \begin{tabular}{|c||c|} \hline
      $\eta$  & $\norm{\w}_2$   \\ \hline
      0.001   & 3.801879        \\ \hline
      0.01    & 9.057170        \\ \hline
      0.05    & 22.291951       \\ \hline
      \end{tabular}
      \caption{The $l_2$ norms of the logistic regression weights after one pass over the data with SGD ($\lambda=0$)}
      \label{tab:weights_2b}
    \end{table}

    Figure \ref{fig:avg_loss_SGD} shows the average loss $\bar L$ as a function of the number of steps $T$ for $\eta = \{0.00, 0.01, 0.05\}$ without regularization. Note that the average losses in each case were identical, making it appear as if only one curve has been plotted. The $l_2$ norms of the weights after the passes are shown in Table \ref{tab:weights_2b}.
    \begin{table}
      \centering
      \begin{tabular}{|c|c|c|} \hline
      $\eta$ & Predicted CTR & Average CTR \\ \hline
      0.001&  0.171172    & 0.173084    \\ \hline
      0.01 &  0.171328    & 0.173084    \\ \hline
      0.05 &  0.173405    & 0.173084    \\ \hline
      \end{tabular}
      \caption{The RMSE of the predicted CTR for the test data using logistic regression optimized with SGD ($\lambda=0$)}
      \label{tab:RMSE_2b}
    \end{table}

    The RMSE on the test set using my predicted CTR and the average CTR for various values of $\eta$ is summarized in Table \ref{tab:RMSE_2b}.

  \end{enumerate}
  
  {\bf 3. Regularization}
  \begin{enumerate}[(a)]
    \item Implement the regularization, and train the weights again using stepsize
      $\eta = 0.05$ with $\lambda$ ranging from $0$ to $0.014$ spaced by $0.002$,
      e.g. $[0, 0.002, 0.004, \ldots, 0.014]$.
      \begin{enumerate}[i.]
        \item Plot the $l_2$ norm of the weights as a function of $\lambda$.
        \item Is there a consistent trend in the $l_2$ norm as $\lambda$ increases? Why does this make sense?
        \item As we increase $\lambda \rightarrow \infty$, what will the $l_2$
          norm converge to?
      \end{enumerate}

    \item Predict the CTR for the test data and evaluate the RMSE.
      Plot the RMSE as a function as $\lambda$.
  \end{enumerate}

  \solution
  \begin{enumerate}[(a)]
    \item \begin{itemize}
      \item[i.] Figure *** shows the $l_2$ norm of the weights as $\lambda$ is increased.
      \item[ii.] ***
      \item[iii.] ***
    \end{itemize}
    \item Figure *** visualizes the RMSE as a function of the regularization parameter $\lambda$.
  \end{enumerate}

  {\bf 4. Hashing Kernel}
  \\
  \\The ``Weinberger, Kilian, et al.'' paper introduces an unbiased hash kernel $\phi: \mathcal{X} \rightarrow \mathcal{F}$.  The original feature space $\mathcal{X}$ is transformed into a space $\mathcal{F}$ with lower dimension through two hash functions: $h: \mathcal{I} \rightarrow \{0, \dots, m-1\}$, and $\xi: \mathcal{I} \rightarrow \{+1, -1\}$, where $\mathcal{I}$ indexes the original feature space $\mathcal{X}$. In this problem, we only ask you to hash the text features, keeping the rest of the features as before. Therefore, $\mathcal{I}$ will be the space of all token ids. 
  \\
  \\The new feature vector (for the text features) $\phi(x)$ will be an $m$-dimensional array, where the $\phi(x)_i = \sum_{j : h(j) = i} \xi(j) X_j$. Now, we can run the same SGD algorithm in the hashed feature space. The sparse updating and lazy regularization tricks still apply.
  \\
  % previously, m = \{97, 12289, 1572869\} (and m=12289 for personalization)
  \\ Train the weights in the hashed feature space with $m = \{101, 12277, 1573549\}$, $\lambda = 0.001$ and stepsize $\eta = 0.01$. Report the RMSE of the predicted CTRs for all 3 cases. 
  

  {\bf 5. Extra Credit: Personalization}
  \\
  \\If you have read and understood the ``Weinberger, Kilian, et al.'' paper in its entirety, you can implement a personalized version of CTR prediction.  It's just a few lines of code to change: Instead of hashing each feature once, you hash it again with the userid. The rest remains the same. 
  \\
  \\Implement the personalized logistic regression with hashing. Train the weights using $\eta = 0.01$, $m = 12277$, and $\lambda = 0.001$.
  \begin{enumerate}[(a)]
    \item Report the RMSE on the test data (including all users).
    \item Report the RMSE just based on the subset of users who appear both in the test and training data.
  \end{enumerate}
  
\end{homeworkProblem}


\end{document}

