\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption,subcaption}
\usepackage{subfig}
\usepackage{enumerate}          % For enumerates indexed by letters
\usepackage{bm}                 % For bold letters
\usepackage{algorithm2e}        % For pseudocode



% 
% Sham's macros
% 
% \input{../macros/gww_defs}
% \input{../macros/gww_chars}
\input{../macros/defs}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass:\ \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Homework 1}
\newcommand{\hmwkDueDate}{April 12, 2016}
\newcommand{\hmwkClass}{CSE 547}
\newcommand{\hmwkAuthorName}{Brian de Silva}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ }\\
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pd}[2]{\frac{\partial}{\partial #1} (#2)}
\newcommand{\pdd}[2]{\frac{\partial #1}{\partial #2}}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\vskip 0.2cm \large Solution:\\}}

% Useful commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand{\bbm}{\begin{bmatrix}}
\newcommand{\ebm}{\end{bmatrix}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\x}{\bm{x}}
\newcommand{\w}{\bm{w}}
\newcommand{\z}{\bm{z}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\norm}[1]{\left\| #1\right\|}


\begin{document}

\maketitle

\pagebreak

\section*{Collaborators} Weston Barger and Emily Dinan.

% Problem 1.1
\begin{homeworkProblem}
    ({\scriptsize Source: KM Exercise 8.6}) {\bf Elementary properties of $l_{2}$ regularized logistic regression}\\
    Consider minimizing

    \begin{flushleft}
    $\hspace{25bp}J(\mathbf{w})=-l(\mathbf{\mathbf{w}},\mathcal{D}_{\textrm{train}})+\lambda\left\Vert \mathbf{w}\right\Vert _{2}^{2}$
    \par\end{flushleft}
    where

    \begin{flushleft}
    $\hspace{25bp}l(\mathbf{\mathbf{w}},\mathcal{D})=\sum_{j}\textrm{ln}\textbf{P}(y^{j}|\mathbf{x}^{j},\mathbf{w})$
    \par\end{flushleft}
    is the log-likelihood on data set $\mathcal{D}$, for $y^{j}\in\{-1,+1\}$.
    Determine whether the following statements are true or false.  Briefly explain.

    \begin{itemize}
        \item[{(a)}] With $\lambda>0$ and the features $x_{k}^{j}$
    linearly separable, $J(\mathbf{w})$ has multiple locally optimal solutions.

        \item[{(b)}] Let $\hat{\mathbf{w}}$= arg min$_{\mathbf{w}}J(\mathbf{w})$ be
    a global optimum. $\hat{\mathbf{w}}$ is typically sparse (has many zero entries).

        \item[{(c)}] If the training data is linearly separable, then some weights $w_{j}$
    might become infinite if $\lambda=$0.
        
        \item[{(d)}] $l(\mathbf{\mathbf{\hat{w}}},\mathcal{D}_{\textrm{train}})$ always increases
    as we increase $\lambda$.

        \item[{(e)}] $l(\mathbf{\mathbf{\hat{w}}},\mathcal{D}_{\textrm{test}})$ always increases
    as we increase $\lambda$.
    \end{itemize}

    \textbf{Now answer the following questions:}
    \begin{itemize}
      \item[{(f)}] Can the decision boundary in unregularized logistic regression
      change if we add an additional variable that is a duplicate of one of the
      variables already in the model?
      Give an intuitive answer (Hint: think about the model assumptions).
      \item[{(g)}] Let us say our $\mathcal{D}_{\textrm{train}}$ has $n$ examples,
      and only one feature $x_1$. Now we create a dataset
      $\mathcal{D}_{\textrm{mod}}$, with $n$ examples and two features $x_1$ and
      $x_2$ where each example in $\mathcal{D}_{\textrm{mod}}$ contains the feature
      on $\mathcal{D}_{\textrm{train}}$ twice, and the same label. We learn a
      logistic regression from $\mathcal{D}_{\textrm{train}}$, which will give us
      two parameters: $w_0$ and $w_1$. We also learn a logistic regression from
      $\mathcal{D}_{\textrm{mod}}$, which will give us three paramters: $w_0'$,
      $w_1'$, $w_2'$.

      \begin{itemize}
      \item[i.] Write down the \textbf{unregularized} log-likelihood we want to maximize for each of the
      two logistic regressions.
      \item[ii.] Given the log-likelihood functions, what is the relationship
      between $(w_0, w_1)$ and $(w_0', w_1', w_2')$? Using this relationship, answer
      question (e) again here, more formally.
      \item[iii.] Would your answer for the previous question change if we were
      using $l_2$ regularization?. Argue why or why not (Remember we don't regularize $w_0$).
      \end{itemize}
    \end{itemize}

    \solution
    \begin{enumerate}[(a)]
        \item False. With $\lambda>0$ and $x_k^j$ linearly separable, both $-l(\bm{w},\mathcal{D})$ and $\lambda\|\bm{w}\|_2^2$ are convex. In fact $\lambda\|\bm{w}\|_2^2$ is strictly convex. Hence $J(\bm{w})$ is convex as well since the sum of convex functions is convex. Since $J(\bm{w})$ is convex. It is easy to show that the strict convexity of $\lambda\|\bm{w}\|_2^2$ implies that $J(\bm{w})$ is also strictly convex. Therefore $J(\bm{w})$ has at most one minimum.
        \item False. Notice that without the 2-norm penalty term it is very unlikely that $\hat \w$ would be sparse as any weight $\hat w_i$ being zero means that the $i$th feature can be safely ignored. Only in very special instances will many such features exist. For the 2-norm regularization term $\lambda \|\w\|_2^2=\lambda\sum_i w_i^2$, weights $w_i$ that are small, but nonzero do not make much of a contribution since they become smaller when squared. In order for the model to prioritize shrinking the size of these weights in optimizing $J(\w)$, $l(\w,\mathcal{D}_{\text{train}})$ must be $\mathcal{O}(\lambda w_i^2)$ as well. However, $l(\w,\mathcal{D})$ is bounded above by 0 and in order for it to actually approach 0, $\|\w\|$ must become large (since we assume $\bm{P}(y|\x,\w))$ is a sigmoid function which never reaches 0 or 1, but can approach them for large $\|\w\|$). But if $\|\w\|$ is large, then there is some $k$ such that $|w_k|$ is large, making $J(\w)$ large. Thus it is very unlikely that the global minimizer for $J(\w)$ has many zero entries.
        \item True. Assume for simplicity that we are not using an offset $w_0$. If the training data is linearly separable then we can find some weight vector $\w$ such that level set $\{\z|\w^T\z=0\}$ separates the points $\x^j$ corresponding to $y^j=-1$ from the points $\x^j$ corresponding to $y^j=1$. Note that this also entails that $\w^T\x^j\neq0$ for each $j$. Then we can make the following observations:
        \begin{enumerate}
          \item $\alpha \w$ also separates the two sets of points for any $\alpha\neq 0$ since $\w^T\z=0$ if and only if $(\alpha\w)^T\z=\alpha(\w^T\z)=0$.
          \item If $\alpha>1$ then $l(\alpha\w,\mathcal{D})>l(\w,\mathcal{D})$. This follows from our definition of $\bm{P}(y^j,\x^j,\w)$:
          \begin{align*}
            \bm{P}(y^j=1|\x^j,\w)&=\frac{1}{1+\text{exp}(-\w^T\x)}\\
            \bm{P}(y^j=-1|\x^j,\w)&=\frac{1}{1+\text{exp}(\w^T\x)}.
          \end{align*}
          This definition implies that
          \[
            l(\alpha\w,\mathcal{D})=-\sum_j\log(1+\text{exp}(-\alpha  y^j\w^T\x)).
          \]
          We can see from here that $l(\alpha\w,\mathcal{D})>l(\w,\mathcal{D})$ if $\alpha>1$.
        \end{enumerate}
          From these observations we see that to minimize $-l(\alpha\w,\mathcal{D}_{\text{train}})$ (hence $J(\w)$) we should let $\alpha\to\infty$. But if any entry of $\w$ is nonzero then this optimal solution will have entries that go to infinity as $\alpha\to\infty$.
        \item False. Minimizing $J(\w)$ is a tradeoff between simultaneously maximizing $l(\w,\mathcal{D}_{\text{train}})$ and minimizing $\lambda\|\w\|_2^2$. As we saw before, if $\lambda=0$ (and the data are linearly separable) then to get $l(\w,\mathcal{D}_{\text{train}})$ to approach its maxima of 0, we often need $\hat \w$ to have infinitely large entries. As we increase $\lambda$ from 0 we are increasingly restricted in how large the entries of $\hat \w$ can be (clearly the minimum is not attained by a $\hat \w$ with infinitely large entries). This imposes increasingly severe restrictions on how well we are able to maximize $l(\w,\mathcal{D}_{\text{train}})$, meaning that $l(\hat\w,\mathcal{D}_{\text{train}})$ will often {\it decrease} as $\lambda$ is increased.
        \item False. Consider the case where $\mathcal{D}_{\text{test}}=\mathcal{D}_{\text{train}}$. Then by the same reasoning as in the previous part, $l(\hat \w,\mathcal{D}_{\text{test}})$ does not necessarily increase as we increase $\lambda$.

        \item Yes. In using logistic regression we assume that the variables are independent. Two variables that are duplicates of one another are clearly dependent, violating this assumption.

        \item 
        \begin{itemize}
          \item[i.] For the first logistic regression the log-likelihood we want to maximize is
          \[
            l(\w,\mathcal{D}_{\text{train}})=-\sum^n_{j=1}\log(1+\text{exp}(-y^j(w_0+w_1x_1^j))).
          \]
          For the second it is
          \[
            l(\w,\mathcal{D}_{\text{train}})=-\sum^n_{j=1}\log(1+\text{exp}(-y^j(w_0'+w_1'x_1^j+w_2'x_2^j))) = -\sum^n_{j=1}\log(1+\text{exp}(-y^j(w_0'+(w_1'+w_2')x_1^j)))
          \]
          as $x_1^j=x_2^j$.
          \item[ii.] We would like to choose the weights that maximize these two log-likelihoods. The expression for the second log-likelihood function is identical to the first, except $w_1$ has been replaced with $w_1'+w_2'$. It follows that if we find weights that maximize the first log-likelihood, $\hat w_0$ and $\hat w_1$, then we can maximize the second by setting $w_0'=\hat w_0$ and $w_1'+w_2'=\hat w_1$.

          In a more general setting where $n$ weights $w_1,w_2,\dots,w_n$ are to be learned (assume for simplicity that we do not use an offset, $w_0$), our decision boundary is the set $\{\z|\w^T\z=0\}$, i.e. the components of any vector on the decision boundary satisfy $z_1w_1+z_2w_2+\dots+z_nw_n=0$. If we introduce an additional variable that is a duplicate of another, say the first, then the new inner product we compute to decide how to classify points is 
          \[
            \tilde w_1 z_1+\tilde w_2z_2+\dots+\tilde w_nz_n+\tilde w_{n+1}z_{n+1}=(\tilde w_1+\tilde w_{n+1}) z_1+\tilde w_2z_2+\dots+\tilde w_nz_n.
          \]
          In order for the decision boundary to stay the same we would need $\tilde w_1+\tilde w_{n+1}=w_1$, showing that***
          \item[iii.] If we use $L^2$ regularization then in the first case we would need to minimize
          \[
            J(\w)=-l(\w,D_{\text{train}})+\lambda\|\w\|_2^2=l(\w,D_{\text{train}})+\lambda w_1^2.
          \]
          Suppose this is minimized by $\w=(w_0,w_1)$. Then for the second case we would need to minimize
          \[
            J(\w)=-l(\w,D_{\text{train}})+\lambda\|\w\|_2^2=l(\w,D_{\text{train}})+\lambda((w_1')^2+(w_2')^2).
          \]
          In the previous part we found that we could take $w_0'=w_0$ and $w_1'+w_2'=w_1$ to find the minimizer. This will make the value of the two log-likelihoods the same, but the $L^2$ terms will not match, as 
          \[
            w_1^2=(w_1'+w_2')^2=(w_1')^2+2w_1'w_2'+(w_2')^2\neq (w_1')^2+(w_2')^2.
          \]
          Hence the relation from the previous part will not necessarily give the minimizer for the second logistic regression model.
        \end{itemize}
    \end{enumerate}

\end{homeworkProblem}


% Problem 1.2
\begin{homeworkProblem}
  On Slide 7 of the first lecture, we presented multi-class logistic regression,
  where $Y \in \{y_1, ..., y_R\}$. Here, we have a simplified version, with no
  $w_0$. When $k < R$, the posterior probability is given by:
  \begin{equation*}
  P(Y = y_k | X) = \frac{exp(\langle w_{k}, X \rangle)}{1 + \sum_{j =
  1}^{R-1}exp(\langle w_{j}, X \rangle)}
  \end{equation*}
  For $k = R$, the posterior is:
  \begin{equation*}
  P(Y = y_k | X) = \frac{1}{1 + \sum_{j = 1}^{R-1}exp(\langle w_j , X \rangle)}
  \end{equation*}
  Where $\langle w_j, X \rangle = \sum_{i=1}^{n}w_{ji}X_i$ (i.e. the dot product).
  We can replace the two equations above by a single equation, to simplify
  notation. For such, we introduce a fixed, pseudo parameter vector $w_R =
  [0,0,0,...,0]$. Now, for any label $y_k$, we write:
  \begin{equation*}
  P(Y = y_k | X) = \frac{exp(\langle w_{k}, X \rangle)}{1 + \sum_{j =
  1}^{R-1}exp(\langle w_{j}, X \rangle)}
  \end{equation*}
  \begin{enumerate}[(a)]
    \item[(a)] How many parameters do we need to estimate? What are these parameters?
    \item[(b)] Given $N$ training samples $\{(x^1, y^1), (x^2, y^2), ..., (x^N,y^N)\}$,
    write down explicitly the log-likelihood function and simplify it as much as you
    can:
    \begin{equation*}
    L(w_1, ..., w_{R-1}) = \sum_{j = 1}^{N}ln(P(y^j| x^j,w))
    \end{equation*}
    \item[(c)] Compute the gradient of $L$ with respect to each $w_k$ and simplify
    it.
    \item[(d)] Now add the regularization term $\dfrac{\lambda}{2}$ and define a new
    objective function:
    \begin{equation*}
    L(w_1, ..., w_{R-1}) = \sum_{j = 1}^{N}ln(P(y^j| x^j,w)) - \frac{\lambda}{2}\sum_{l=1}^{R-1}\norm{w_l}_2^2
    \end{equation*}
    Compute the gradient of this new $L$ with respect to each $w_k$.
  \end{enumerate}

  \solution
  \begin{enumerate}[(a)]
    \item The parameters we need to estimate are the entries of each of the vectors $w_1,w_2,\dots,w_{R-1}$. Since each of these has $n$ entries, we must estimate $(R-1)n$ parameters total.
    \item For this problem we use the convention introduced above that $w_R=[0,0,\dots,0]$.
    \begin{align*}
      L(w_1,\dots,w_{R-1})&=\sum_{j=1}^N\log(\bm{P}(y^j|x^j,w))\\
      &= \sum_{j=1}^N\log\left(\frac{exp(\langle w_{y^j}, x^j \rangle)}{1 + \sum_{i=1}^{R-1}exp(\langle w_{i}, x^j \rangle)} \right)\\
      &= \sum_{j=1}^N\left(\log(exp(\langle w_{y^j},x^j\rangle))-\log\left(1+\sum^{R-1}_{i=1}exp(\langle w_i,x^j\rangle)\right)\right)\\
      &=\sum^N_{j=1}\left( \langle w_{y^j},x^j\rangle -\log\left(\sum^{R}_{i=1}exp(\langle i_j,x^j\rangle) \right)\right).
    \end{align*}
    \item Let $\bm{1}$ denote the identity function defined as
    \[
      \bm{1}(y=k)=\begin{cases}
        1 &\text{if~}y=k\\
        0 & \text{if~}y\neq k.
      \end{cases}
    \]
    Suppose $k\neq R$. Then the gradient of one log likelihood term with respect to $w_k$ is
    \begin{align*}
      \frac{\partial }{\partial w_k}\log(\bm{P}(y^j|x^j,w)) &= \left(\frac{\partial }{\partial w_k}\langle w_{y^j},x^j\rangle - \frac{\partial }{\partial w_k}\log\left(\sum^{R}_{i=1}exp(\langle w_i,x^j\rangle) \right)\right)\\
      &= \bm{1}(y^j=k)x^j - (1-\bm{1}(y^j=R))\frac{x^jexp(\langle w_k,x^j\rangle)}{\sum^R_{i=1}\langle w_i,x^j\rangle} \\
      &= x^j\left(\bm{1}(y^j=k) -(1-\bm{1}(y^j=R))\frac{exp(\langle w_k,x^j\rangle)}{\sum^R_{i=1}\langle w_i,x^j\rangle}  \right).
    \end{align*}
    We can also write this gradient as
    \[
       \frac{\partial }{\partial w_k}\log(\bm{P}(y^j|x^j,w)) = \begin{cases}
         x^j(\bm{1}(y^j=k) -\bm{P}(y^j=k|x^j,w))\\
         % \frac{exp(\langle w_k,x^j\rangle)}{\sum^R_i=1}\langle w_i,x^j\rangle}  \right) & \text{if~} y^j\neq R\\
         x^j\bm{1}(y^j=R) & \text{if~} y^j=R
       \end{cases}.
    \]
    If $k=R$ then the gradient is the zero vector since $w_R$ is a pseudo parameter vector which is fixed. Now we can easily write down the gradient of $L(w)$ with respect to $w_k$ (for $k\neq R$):
    \begin{align*}
      \frac{\partial }{\partial w_k}L(w_1,w_2,\dots,w_{k-1}) &= \frac{\partial }{\partial w_k}\sum_{j=1}^N\log(\bm{P}(y^j|x^j,w))\\
      &=\sum_{j=1}^N \frac{\partial }{\partial w_k} \log(\bm{P}(y^j|x^j,w))\\
      &=\sum_{j=1}^N x^j\left[\bm{1}(y^j=k) -(1-\bm{1}(y^j=R))\bm{P}(y^j|x^j,w)  \right].
    \end{align*}
    \item This new objective function is the same as the previous one except it includes an $L^2$ regularization term. Taking the gradient of this term with respect to $w_k$ (for $k\neq R$) gives
    \[
      -\frac{\partial }{\partial w_k}\frac\lambda2\sum^{R-1}_{l=1}\norm{w_l}_2^2 = -\lambda w_k.
    \]
    Hence the gradient of $L$ with respect to $w_k$ is
    \[
      \frac{\partial }{\partial w_k}L(w_1,w_2,\dots,w_{k-1}) = x^j\left[\bm{1}(y^j=k) -(1-\bm{1}(y^j=R))\bm{P}(y^j|x^j,w)  \right] - \lambda w_k.
    \]
  \end{enumerate}
\end{homeworkProblem}






\end{document}

