\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption,subcaption}
\usepackage{subfig}
\usepackage{enumerate}          % For enumerates indexed by letters
\usepackage{bm}                 % For bold letters
\usepackage{algorithm2e}        % For pseudocode
\usepackage{url}                % So texttt wraps instead of creating hbox

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=7in
\textheight=9.0in
\headsep=0.25in
\linespread{1.1}
\pagestyle{fancy}

\lhead{\hmwkAuthorName}
\chead{\hmwkClass:\ \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{CSE 547 Homework 2}
\newcommand{\hmwkDueDate}{April 26, 2017}
\newcommand{\hmwkClass}{CSE 547}
\newcommand{\hmwkAuthorName}{Brian de Silva}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ }\\
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pd}[2]{\frac{\partial}{\partial #1} (#2)}

\newcommand{\pdd}[2]{\frac{\partial #1}{\partial #2}}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\vskip 0.2cm \large Solution:\\}}

% Useful commands
\newcommand{\bbm}{\begin{bmatrix}}
\newcommand{\ebm}{\end{bmatrix}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\dtdx}{\frac{\Delta t}{\Delta x}}
\newcommand{\half}{\frac12}
\newcommand{\norm}[1]{\left\|#1\right\|}
% \newcommand{\Pr}{\mathbb{P}}


\begin{document}

\maketitle

\pagebreak

\section*{Collaborators}
I collaborated with Weston Barger and Emily Dinan on ***

% Problem 1
\begin{homeworkProblem}
	{\bf Gaussian Random Projections and Inner Products [10 Points]}
	\\
	\\
	In this problem, you will show that inner products are approximately preserved using random projections. Let $\phi(x) = \tfrac{1}{\sqrt{m}}Ax$ represent our random projection of $x \in \mathbb{R}^d$, with $A$ an $m \times d$ projection matrix with each entry sampled i.i.d from $N(0,1)$. (Note that each row of $A$ is a random projection vector, $v^{(i)}$.)  

	The \emph{norm preservation theorem} states that for all $x \in \mathbb{R}^d$, the norm of the random projection $\phi(x)$ approximately maintains the norm of the original $x$ with high probability:
	\begin{equation}
	\label{eq:npt}
	\Pr\left((1 - \epsilon) \norm{x}^2 \leq \norm{\phi(x)}^2 \leq (1 +
	\epsilon)\norm{x}^2 \right) \geq 1 - 2e^{-(\epsilon^2 -\epsilon^3)m/4},
	\end{equation}
	where $\epsilon \in (0, 1/2)$. 

	Using the norm preservation theorem, prove that for any $u,v \in \mathbb{R}^d$ s.t. $\norm{u} \leq 1$ and $\norm{v} \leq 1$,
	\begin{equation}
	\Pr(|u\cdot v - \phi(u)\cdot\phi(v)| \geq \epsilon) \leq 4e^{-(\epsilon^2 -\epsilon^3)m/4}.
	\end{equation}
	Note that $u\cdot v$ is the original dot product, and $\phi(u)\cdot \phi(v)$ is the dot product for the random projections. This statement puts a probabilistic bound on the distance between the two dot products. (\emph{Hint: Think about using Theorem (\ref{eq:npt}) with $x = u+v$ and $x = u-v$}).

	\solution

	First notice that
	\begin{align*}
		&(1-\epsilon)\|x\|^2\leq \norm{\phi(x)}^2\leq (1+\epsilon)\norm{x}^2\\
		\iff& \norm{x}^2-\epsilon\norm{x}\leq \norm{\phi(x)}^2\leq \norm{x}^2+\epsilon\norm{x}^2\\
		\iff & -\epsilon\norm{x}^2\leq \norm{\phi(x)}^2-\norm{x}^2\leq \epsilon\norm{x}^2\\
		\iff & \left|\norm{\phi(x)}^2-\norm{x}^2 \right|\leq \epsilon\norm{x}^2.
	\end{align*}
	Multiplying the above by $-1$ gives a similar statement:
	\[
		-\epsilon\norm{x}^2\leq \norm{x}^2- \norm{\phi(x)}^2\leq \epsilon\norm{x}^2.
	\]
	Observe also that $\phi$ is a linear function, so for any $x,y\in\R^d$, $\phi(x+y)=\phi(x)+\phi(y)$. We will need this fact later.
	Following the hint and applying Theorem (\ref{eq:npt}) to $x=u+v$ and $x=u-v$, we obtain
	\[
		\Pr\left((1-\epsilon)\|u+v\|^2\leq \norm{\phi(u+v)}^2\leq (1+\epsilon)\norm{u+v}^2\right) \geq 1-2e^{-(\epsilon^2-\epsilon^3)m/4}
	\]
	and
	\[
		\Pr\left((1-\epsilon)\|u-v\|^2\leq \norm{\phi(u-v)}^2\leq (1+\epsilon)\norm{u-v}^2\right) \geq 1-2e^{-(\epsilon^2-\epsilon^3)m/4}.
	\]
	Manipulating the inequalities as above, these two statements become
	\[
		\Pr\left(\left|\norm{\phi(u+v)}^2-\norm{u+v}^2 \right|\leq \epsilon\norm{u+v}^2 \right)\geq 1-2e^{-(\epsilon^2-\epsilon^3)m/4}
	\]
	and
	\[
		\Pr\left(\left|\norm{\phi(u-v)}^2-\norm{u-v}^2 \right|\leq \epsilon\norm{u-v}^2 \right)\geq 1-2e^{-(\epsilon^2-\epsilon^3)m/4}.
	\]
	These expressions are equivalent to the following
	\[
		\Pr\left(\left|\norm{\phi(u+v)}^2-\norm{u+v}^2 \right|\geq \epsilon\norm{u+v}^2 \right)\leq 1 - \left(1-2e^{-(\epsilon^2-\epsilon^3)m/4}\right) = 2e^{-(\epsilon^2-\epsilon^3)m/4}
	\]
	and
	\[
		\Pr\left(\left|\norm{\phi(u-v)}^2-\norm{u-v}^2 \right|\geq \epsilon\norm{u-v}^2 \right)\leq 1 - \left(1-2e^{-(\epsilon^2-\epsilon^3)m/4}\right) = 2e^{-(\epsilon^2-\epsilon^3)m/4}.
	\]

	Now, recall that for any two events $A$ and $B$, $\Pr(A\wedge B)\leq \Pr(A)+\Pr(B)$. Letting $A$ be the event that $$A:~~\left|\norm{\phi(u+v)}^2-\norm{u+v}^2 \right|\geq \epsilon\norm{u+v}^2$$ and $B$ be the event $$B:~~\left|\norm{\phi(u-v)}^2-\norm{u-v}^2 \right|\geq \epsilon\norm{u-v}^2,$$ it follows that the probability of both occuring is at most $\Pr(A)+\Pr(B)\leq4e^{-(\epsilon^2-\epsilon^3)m/4}$. We saw previously that 
	\begin{align*}
		&\left|\norm{\phi(u+v)}^2-\norm{u+v}^2 \right|\geq \epsilon\norm{u+v}^2\\
		\iff & -\epsilon\norm{u+v}^2\leq \norm{u+v}^2-\norm{\phi(u)+\phi(v)}^2\leq \epsilon\norm{u+v}^2
	\end{align*}
	and 
	\begin{align*}
		&\left|\norm{\phi(u-v)}^2-\norm{u-v}^2 \right|\geq \epsilon\norm{u-v}^2\\
		\iff & -\epsilon\norm{u-v}^2\leq \norm{\phi(u)-\phi(v)}^2 - \norm{u-v}^2\leq \epsilon\norm{u+v}^2.
	\end{align*}
	If both inequalities hold simultaneously, we may add them together to obtain
	\[
		-\epsilon\left(\norm{u+v}^2+\norm{u-v}^2\right) \leq \norm{\phi(u)-\phi(v)}^2-\norm{\phi(u)+\phi(v)}^2 + \norm{u+v}^2-\norm{u-v}^2\leq\epsilon\left(\norm{u+v}^2+\norm{u-v}^2\right).
	\]
	Using the Parallelogram law and Polarization identity, we can simplify this to
	\begin{align*}
		&-2\epsilon\left(\norm{u}^2+\norm{v}^2\right)\leq 4\left(u\cdot v-\phi(u)\cdot\phi(v) \right) \leq 2\epsilon\left(\norm{u}^2+\norm{v}^2\right)\\
		\iff& \epsilon\left(\norm{u}^2+\norm{v}^2\right)\leq 2\left(u\cdot v-\phi(u)\cdot\phi(v) \right) \leq \epsilon\left(\norm{u}^2+\norm{v}^2\right) \\
		\implies & 2\left|u\cdot v-\phi(u)\cdot\phi(v) \right|\leq \epsilon \left(\norm{u}^2+\norm{v}^2\right) \leq \epsilon(1+1) = 2\epsilon\\
		\implies& \left|u\cdot v-\phi(u)\cdot\phi(v) \right| \leq \epsilon.
	\end{align*}
	Putting everything together, we have our result
	\begin{align*}
		\Pr\left(\left|u\cdot v-\phi(u)\cdot\phi(v) \right| \leq \epsilon \right) \leq 4e^{-(\epsilon^2-\epsilon^3)m/4}.
	\end{align*}
\end{homeworkProblem}
\newpage

% Problem 2
\begin{homeworkProblem}
  {\bf KD-Trees and LSH for Approximate Neighbor Finding [15 Points]}
  \\
  \\In this problem, you will use KD-trees and Locality Sensitive
  Hashing (LSH) to find exact and approximate nearest neighbors. To
  explore the performance of KD-trees, you can use the starter code in
  ``NearestNeighbor.zip'' on the course website. You may alter the class
  \texttt{KDTreeAnalysis} to generate a dataset and return the time it
  takes to query from the KD-tree on that dataset, although this is not a
  part of the assignment.
  For approximate nearest neighbor search, $\alpha$ can
  be set to a value greater than 1. The KD-tree implementation was
  downloaded from\\http://sourceforge.net/projects/java-ml.

  For datasets with high dimensionality $d$, KD-trees will not scale
  well and other methods must be used. LSH offers a method for
  reducing the dimensionality of the data while still enabling
  approximate neighbor finding. For this portion of the problem, you
  will be using an artificially generated dataset consisting of
  term-document frequency for a vocabulary of size $d = 1000$ and $n =
  100000$ documents. The data can be found in ``sim\_docdata.zip'' on
  the course website. The zip file consists of two files,
  sim\_docdata.mtx and test\_docdata.mtx, which contain the term
  frequencies in a sparse matrix in Matrix Market format: each row is
  in the form ``termid docid frequency''. test\_docdata.mtx contains a
  set of documents to use for querying nearest neighbors. The size of
  the test set is 500 documents. You will need to complete the
  classes \texttt{MethodComparison},
  \texttt{GaussianRandomProjection}, and
  \texttt{LocalitySensitiveHash}.
  \begin{enumerate}[(a)]
  \item Implement a nearest neighbor search using
    LSH and $m$, the number of projections, in $\{5, 10,
    20\}$. Although normally we would search 'until time runs out',
    for this problem, just search all bins that have a Hamming
    distance from the bin of the query data point $<= 3$. Record the
    average query time and the average distance to the nearest
    neighbor for the test set. Compare this with the average query
    time and average distance using a KD-tree with $\alpha$ in $\{1,
    5, 10\}$. Note that the KD-tree implementation will be slow, and
    may take 5-10 minutes. Explain the trends found.
  \item Implement a Gaussian random projection on
    the document data for $m$ in $\{5, 10, 20\}$. Use these projections
    as the entries for a KD-tree. This results in an alternative approximate
    nearest neighbor search rather than using $\alpha > 1$. Again,
    record the average query time and average distance over the test
    set, and explain the trends found.
  \end{enumerate}

  \solution

  \begin{enumerate}[(a)]
  	\item 
  	\begin{table}[h]
  		\centering
  		\begin{tabular}{|c|c|c|}\hline
  		    {\bf Number of projections} & {\bf Average query time (s)} & {\bf Average distance to NN}\\ \hline
  		    5		&0.482571		&4.832050		\\ \hline
  		    10		&0.105674		&5.356869		\\ \hline
  		    20		&0.054416		&$\infty$		\\ \hline
  		\end{tabular}
  		\caption{Statistics for approximate nearest neighbor search using a location sensitive hash table with different numbers of projections.}
  		\label{tab:lsh}
  	\end{table}

  	\begin{table}[h]
  		\centering
  		\begin{tabular}{|c|c|c|}\hline
  		    {\bf $\alpha$} & {\bf Average query time} & {\bf Average distance to NN}\\ \hline
  		    1		&63.911429	&4.455368		\\ \hline
  		    5		&22.306609	&4.782051		\\ \hline
  		    10		&19.210534	&4.852166		\\ \hline
  		\end{tabular}
  		\caption{Statistics for approximate nearest neighbor search using a KD-tree different pruning parameters $\alpha$.}
  		\label{tab:kdtree}
  	\end{table}

  	Tables \ref{tab:lsh} and \ref{tab:kdtree} show the average query times and average distances to the approximate nearest neighbors for the locality sensitive hashing and KD-tree implementations of the approximate nearest neighbor problem, respectively. As expected, the average query time using a locality sensitive hash table is {\it much} faster than that of the KD-tree. However, the KD-tree finds closer points to the input points on average. For $\alpha=1$ this is to be expected since it finds the actual nearest neighbor. As $\alpha$ is increased the query time decreases as the tree can prune more aggressively, but the quality of the neighbor is diminished marginally.

  	For the implementation involving locality sensitive hashing, the average query time decreases as a function of the number of projections used. This is because if one uses $m$ projections, then one has $2^m$ bins in which to place objects. If $m$ is small then there are many fewer bins than there are objects in the training set, so bins can contain a large number of objects. For a fixed depth, when a query is made one has to search a fixed number of bins for a nearest neighbor. If each of these bins contains many objects then the time one must spend checking them for nearest neighbors grows (since one must compute the distance between the reference object and each of the objects in these bins). If we use more projections there are more bins over which the objects are distributed, so each bin contains fewer objects on average. This leads to faster query times on average.

  	The average distance to the returned nearest neighbor grows with the number of projections for the LSH implementation. This can be explained by similar reasoning to before. For small $m$ lots of objects fall in the same bin. When we look for a nearest neighbor we are likely to search over a larger set of objects, often allowing us to find one that is closer to the reference object than we would have found for a larger value of $m$. Note that for $m=20$ the average distance to the nearest neighbor is $\infty$. The $\infty$ just means that for at least one reference point, no nearest neighbor was located (using a depth of 3). This is because there are $2^20$ bins, while only 100000 training objects (so there are about 10 times as many bins as objects). Many bins will be empty. There is some object in the test set which gets mapped to a bin with no nonempty bins within a distance of 3 of it (using Hamming-distance). If we were to increase the depth above 3 it is possible that a neighbor would be found.

  	\item
  	\begin{table}[h]
  		\centering
  		\begin{tabular}{|c|c|c|} \hline
  		    {\bf Number of projections} & {\bf Average query time (s)} & {\bf Average distance to NN}\\ \hline
  		    5		&0.003674		&7.391327		\\ \hline
  		    10		&0.074750		&7.117428		\\ \hline
  		    20		&1.420972		&6.301496		\\ \hline
  		\end{tabular}
  		\caption{Statistics for approximate nearest neighbor search using Gaussian random projections with different numbers of projections fed into a KD-tree.}
  		\label{tab:grp}
  	\end{table}
  	Table \ref{tab:grp} gives the average query time and average distance over the test set using Gaussian random projections. The first observation to make is that the average query time grows with the number of projections used. The number of projections is the dimension of the data points inserted into the KD-tree. Lookups become much slower for KD-trees containing higher dimensional objects, which explains this trend in the average query time. The average distance to the nearest neighbor that is returned decreases as the number of projections increases. This is because the more random projections that are used the more information about the original object that is captured. Given a reference object the KD-tree will return its true nearest neighbor {\it with respect to their representations in the projected space}. Consider the extreme case where only one projection is used. The KD-tree will compute distances between 1-dimensional scalars which are proxies for the true $d$-dimensional objects. If $d$ is large then much information is lost when projecting these objects down to 1 dimension. As the dimension of the projected space grows we expect to capture more and more information about the original objects. Thus the KD-tree is better able to find nearest neighbors using these richer (higher-dimensional) representations than the lower-dimensional ones.

  \end{enumerate}
\end{homeworkProblem}

\begin{homeworkProblem}
	{\bf The Clustering Toolkit: K-Means and EM}
	\\
	\\In this problem, you will implement the k-means algorithm and EM to fit a Gaussian Mixture Model (GMM). This problem consists of 3 parts. In part 1, you will implement the two clustering algorithms and
	evaluate on a 2D synthetic dataset. In part 2, we will provide you
	with a small text dataset (from the BBC). Your task is to 
	implement (or reuse) the same algorithms to cluster the documents. In the last part, you will implement the k-means 
	algorithm in Hadoop MapReduce, and test your algorithm against a
	subset of a Wikipedia dataset.
	\\
	\\{\bf 1. 2D synthetic data}
	\\
	\\The k-means algorithm is an intuitive way to explore the structure
	of a dataset. Suppose we are given points $x^{1}$, ..., $x^{n}$ $\in R^{2}$
	and an integer $K > 1$, and our goal is to minimize the within-cluster sum of squares
	\[
	J(\mu,\mathcal{Z})=\sum_{i=1}^{n}\left\|x^{i}-\mu_{z^{i}}\right\|_2^2,
	\]
	where $\mu=(\mu_{1},...,\mu_{K})$ are the cluster centers with the same
	dimension of data points, and $\mathcal{Z}=(z^{1},...,z^{n})$ are the cluster
	assignments, $z^{i}\in\{1,...,K\}$. One common algorithm for finding an 
	approximate solution is Lloyd's algorithm. To apply the 
	Lloyd's k-means algorithm one takes a guess at the number of clusters 
	(i.e., select a value for $K$) and initializes cluster centers $\mu_1,\ldots,\mu_K$ by 
	picking $K$ points (often randomly from $x^{1},...,x^{n}$).
	In practice, we often repeat multiple runs of Lloyd's algorithm with
	different initializations, and pick the best resulting 
	clustering in terms of the k-means objective. The algorithm then proceeds
	by iterating through two steps:

	\begin{enumerate}[(i)]
	\item Keeping $\mu$ fixed, find the cluster classification $\mathcal{Z}$ to minimize
	$J(\mu,\mathcal{Z})$ by assigning each point to the cluster to which it is closest. 

	\item Keeping $\mathcal{Z}$ fixed, find new centers of the clusters $\mu$ for
	the $(m+1)^{th}$ step to minimize $J(\mu,\mathcal{Z})$ by averaging points within
	a cluster from the points in a cluster at the $m^{th}$ step.
	\end{enumerate}

	Terminate the iteration to settle on $K$ final clusters if applicable.

	\begin{enumerate}[(a)]
		\item Assuming that the tie-breaking rule used in step (i) is consistent,
		does Lloyd's algorithm always converge in a finite number of steps?
		Briefly explain why.

		\item Implement Lloyd's algorithm on the two dimensional data points
		in data file ``2DGaussianMixture.csv''. Run it with some of your randomly
		chosen initialization points with different values of $K \in \{2, 3,
		5, 10,15, 20\}$, and show the clustering plots by color. 

		\item Implement Lloyd\textquoteright{}s algorithm. Run it 20 times,
		each time with different initialization of $K$ cluster centers picked
		at random from the set $\{x^{1},...,x^{n}\}$, with $K = 3$ clusters,
		on the two dimensional data points in data file 2DGaussianMixture.csv (Link is the ``Synthetic Data'' in the homework section). 
		Plot in a single figure the original data (in gray), and all $20\times3$
		cluster centers (in black) given by each run of Lloyd\textquoteright{}s
		algorithm. Also, compute the minimum, mean, and standard deviation of the within-cluster
		sums of squares for the clusterings given by each of the 20 runs.

		\item  K-means++ is another initialization algorithm for K-means (by David
		Arthur and Sergei Vassilvitskii). The algorithm proceeds as follows:
		\begin{enumerate}[(i)]
			\item Pick the first cluster center $\mu_{1}$ uniformly at random from
			the data $x^{1},...,x^{n}$. 
			\item For $j=2,...,K:$ 
				\begin{itemize}
				\item For each data point, compute its distance $D_{i}$ to the nearest
				cluster center picked in a previous iteration:
			\[
			D_{i}=\min_{j^{\prime}=1,...,j-1}\left\Vert x^{i}-\mu_{j^{\prime}}\right\Vert .
			\]
				\item Pick the cluster center $\mu_{j}$ at random from $x^{1},...,x^{n}$
				with probabilities proportional to $D_{1}^{2},...,D_{n}^{2}$. 
				\end{itemize}
			\item Return $\mu$ as the initial cluster assignments for Lloyd\textquoteright{}s
			algorithm.
		\end{enumerate}  
		Replicate Part (c) using k-means++ as the initialization
		algorithm, instead of picking $\mu$ uniformly at random.

		\item Based on your observations in Part (c) and Part (d), is Lloyd's algorithm
		sensitive to initialization? 

		\item One shortcoming of k-means is that one has to specify the value
		of $K$. Consider the following strategy for picking $K$ automatically:
		try all possible values of $K$ and choose $K$ that minimizes $J(\mu,\mathcal{Z})$.
		Argue why this strategy is a good/bad idea. Suggest an alternative
		strategy.

		\item The two-dimensional real data in the file ``2DGaussianMixture.csv''
		are generated from a mixture of Gaussians with three components. Implement
		EM for general mixtures of Gaussians (not just k-means).  Initialize
		the means with the same procedure as in the k-means++ case.
		Initialize the covariances with the identity matrix.  Run your
		implementation on the synthetic dataset, and: 
		\begin{enumerate}[(i)]

			\item Plot the likelihood of the data over EM iterations.  

			\item After convergence, plot the data, with the most likely
			   cluster assignment of each data point indicated by its color.  Mark
			   the mean of each Gaussian and draw the covariance ellipse for each Gaussian.  

			\item How do these results compare to those obtained by k-means?  Why?

		\end{enumerate}

	\end{enumerate}

	\solution 

	\begin{enumerate}[(a)]
		\item Yes, assuming a consistent deterministic tie-breaking rule is used, Lloyd's algorithm will always converge in a finite number of steps. Note that there are only a finite number of ways to group the data points into $K$ clusters. These groupings completely determine the associated means. Further, observe that each step of Lloyd's algorithm which assigns at least one point to a new cluster decreases the value of $J(\mu,\mathcal{Z})$. Since there are only finitely many ways to assign the points to clusters, $J(\mu,\mathcal{Z})$ can only be decreased a finite number of times. Assuming a consistent deterministic tie-breaking rule is used to assign points equidistant from two cluster centers to clusters, once $J(\mu,\mathcal{Z})$ stops decreasing the algorithm will stop reassiging points to new clusters.

		\item
		\begin{figure}
			\centering
			\begin{subfigure}[b]{0.32\textwidth}
				\centering
				\includegraphics[width=.95\linewidth]{figures/kmeans_2-3b_k2}
			\end{subfigure}
			\begin{subfigure}[b]{0.32\textwidth}
				\centering
				\includegraphics[width=.95\linewidth]{figures/kmeans_2-3b_k3}
			\end{subfigure}
			\begin{subfigure}[b]{0.32\textwidth}
				\centering
				\includegraphics[width=.95\linewidth]{figures/kmeans_2-3b_k5}
			\end{subfigure}
			\begin{subfigure}[b]{0.32\textwidth}
				\centering
				\includegraphics[width=.95\linewidth]{figures/kmeans_2-3b_k10}
			\end{subfigure}
			\begin{subfigure}[b]{0.32\textwidth}
				\centering
				\includegraphics[width=.95\linewidth]{figures/kmeans_2-3b_k15}
			\end{subfigure}
			\begin{subfigure}[b]{0.32\textwidth}
				\centering
				\includegraphics[width=.95\linewidth]{figures/kmeans_2-3b_k20}
			\end{subfigure}
			\caption{Clusters determined by K-means applied to the 2D data from {\tt 2DGaussianMixture.csv} for $K=2,3,5,10,15,20$.}
			\label{fig:kmeans3b}
		\end{figure}
    Figure \ref{fig:kmeans3b} shows the clusterings generated using various values of $K$.

    \item 
    \begin{figure}
      \centering
      \includegraphics[width=.55\textwidth]{figures/kmeans_2-3c_centers}
      \caption{The centers (denoted by `x') generated during 20 runs of Lloyd's algorithm with random initializations.} 
      \label{fig:kmeans_2-3c_centers}
    \end{figure}

    Figure \ref{fig:kmeans_2-3c_centers} visualizes the centers chosen by the 20 runs of Lloyd's algorithm with random initializations. The minimum, mean, and standard deviation of the within-cluster sums of squares for the 20 runs were 6.184172, 8.340745, and 3.735294, respectively.

    \item 
    \begin{figure}
      \centering
      \includegraphics[width=.55\textwidth]{figures/kmeans_2-3d_centers}
      \caption{The centers (denoted by `x') generated during 20 runs of Lloyd's algorithm with K-means++ initializations.} 
      \label{fig:kmeans_2-3d_centers}
    \end{figure}

    Figure \ref{fig:kmeans_2-3d_centers} shows the centers chosen by the 20 runs of Lloyd's algorithm with centers initialized with K-means++. The minimum, mean, and standard deviation of the within-cluster sums of squares for the 20 runs were 6.184172, 7.046801, and 2.587888, respectively.

    \item It seems that Lloyd's algorithm is sensitive to initialization. The K-means++ initialization procedure produced better results on average than the random initialization.

    \item This is a bad strategy because the larger $K$ gets the smaller $J(\mathcal{Z},\mu)$ can get. With lots of centers it becomes easy to place centers very close to all the points. In the extreme case where $K$ is equal to the number of points, $J(\mathcal{Z},\mu)$ can be made to be 0 by choosing each point as a center. On the other hand, $K=1$, the center will be the mean of all the points and $J(\mathcal{Z},\mu)$ will be large unless the points are all very close together to begin with.

    A better strategy would be to experiment with a range of reasonable values of $K$ and track how robust each is to differences in initialization. For each $K$ would could run K-means multiple times on the same data set (using different initializations of the centers) and track how different the resulting clusters are between runs. One could track the similarity of clusters either by checking how similar the clusters are to one-another from run to run, or perhaps by observing fluctuations in $J(\mathcal{Z},\mu)$. If one has selected a ``good'' value of $K$ we might expect the algorithm to output somewhat similar clusters each time it is run. Thus we could select the value of $K$ for which the clusters output by K-means showed the most robustness.

    \item My implementation of EM for GMMs terminated after the log-likelihood failed to change by more than $10^{-6}$ between successive iterations.
    \begin{enumerate}[(i)]
      \item 
      \begin{figure}
        \centering
        \includegraphics[width=.55\textwidth]{figures/EM_likelihood_2-3g}
        \caption{The log-likelihood of the data as a function of EM iterations.} 
        \label{fig:EM_likelihood_2-3g}
      \end{figure}
      Figure \ref{fig:EM_likelihood_2-3g} shows the {\bf log-likelihood} of the data over EM iterations.

      \item
      \begin{figure}
        \centering
        \includegraphics[width=.55\textwidth]{figures/EM_2-3g}
        \caption{The most likely cluster assignments after termination of the EM algorithm. The means of the clusters are denoted by `x.' The 95\% confidence intervals are shown as solid curves.} 
        \label{fig:EM_2-3g}
      \end{figure}
      Figure \ref{fig:EM_2-3g} plots the most likely cluster assignments of the data points along with the cluster centers and the covariance ellipses for each Gaussian.

      \item This algorithm performed slightly better than K-means with the correct value of $K$ (see the $K=3$ plot of Figure \ref{fig:kmeans3b}). This is because the two points which K-means misclassifies are both technically closer to the wrong center. K-means does not take the distribution of same-cluster points into account. Since GMM does, it correctly classifies the points. Based on the covariance ellipses shown in Figure \ref{fig:EM_2-3g}, it is much more clear that these two points belong in the cluster on the left rather than the one on the right. Since the GMM algorithm takes this into account, it puts the points in their correct cluster.
    \end{enumerate}


	\end{enumerate}

	
	{\bf 2. Clustering the BBC News  [20 Points]}
	\\
	The dataset we will consider comes from the BBC (\url{http://mlg.ucd.ie/datasets/bbc.html}).
	The preprocessed dataset consists of the term-document frequency of 99 vocabulary and 1791 documents chosen from 5 categories: business, entertainment, politics, sport and tech.

	From lecture, we learned that the term frequency vector is not a good metric because of its biases to frequent terms. 
	Your first task is to convert the term frequency into tfidf using the following equations:

	\begin{eqnarray}
	  \text{tf}(t, d) &=&   \frac{f(t,d)}{max \{f(w,d): (w \in d)\}} \\
	  \text{idf}(t, D) &=&  \log \frac{|D|}{|\{d \in D : t \in d\}|} \\
	  \text{tfidf}(t,d,D) &=& tf (t,d) \times idf(t,D)
	\end{eqnarray}
	\begin{enumerate}[(a)]
	  \item Convert the term-doc-frequency matrix into a term-doc-tfidf matrix.
	    For each term $t$, take the average tfidf over each class $C_i
	    =\{\mbox{documents in class $i$}\}$: 
	    \[
	    \text{avg\_tfidf} (t, C_i, D) = \frac{1}{|C_i|}\sum_{d \in C_i} \text{tfidf}(t,d,D)
	    \] 
	    \textbf{For each class $C_i$, report the 5 terms with the highest $AvgTfidf$ for the class (e.g Tech: spywar:0.69, aol:0.58, $\dots$ Business: $\dots$).} 

	  \item Run k-means with $K = 5$ for 5 iterations, using the centers in ``*.centers'' for initialization.\\
	    \textbf{Plot the classification error (0/1 loss) versus the number
	      of iterations.}  \emph{Note: Don't use the optimal mapping
	      procedure from the previous question; you should keep the
	      class ids as they were in the initialization file.}

	  \item Run EM with $K = 5$ for 5 iterations. Using ``*.centers'' as the mean and identity as the covariance of the initial clusters. Initialize $\pi_{1,\dots,K}$ uniformly.\\
	    You need to be careful when updating the covariance matrix $\Sigma_k$ during the M-step. In particular, the MLE can be ill-conditioned because the data is sparse. To handle this case,
	    we can perform a shrinkage on the MLE: $\hat{\Sigma} =  (1-\lambda)\hat{\Sigma}_{MLE} + \lambda I$, which is equivalent to a MAP estimate of the posterior distribution with some prior.
	    In this problem, please use $\lambda = 0.2$. \\
	    \textbf{Plot the classification error (0/1 loss) versus number of iterations.} \\
	    \textbf{Plot the log-likelihood versus the number of iterations.}
	\end{enumerate}

	\solution

  \begin{enumerate}[(a)]
    \item 
      \begin{table}[h]
        \centering
        \begin{tabular}{|c|c||c|c||c|c||c|c||c|c|} \hline
            {\bf Class 0} & & {\bf Class 1} &  & {\bf Class 2} & & {\bf Class 3} & & {\bf Class 4}   \\ \hline
            wider &     0.7317  & publish     & 0.5767  & misdirect &   0.8668  & venic  &        0.7518  &   spywar  & 0.5942  \\ \hline
            observ  &   0.6131  &   paramount & 0.5519  & float &       0.5538  & seychel &       0.4871  &   aol & 0.5685  \\ \hline
            decid &     0.4212  &   kelli     & 0.5211  & gerhard &     0.4623  & shoe  &         0.4553  &   175 & 0.4589  \\ \hline
            stabilis  & 0.2782  &   hodgson   & 0.5054  & beith &       0.4221  & vicechairman  & 0.3791  &   xbox  & 0.3765  \\ \hline
            unit  &     0.2688  &   coldplai  & 0.4762  & reimburs  &   0.4221  & coral &         0.3094  &   restat  & 0.3467  \\ \hline
        \end{tabular}
      \caption{The five terms with the highest average tf-idf for each of the five classes in the BBC dataset.}
      \label{tab:avg_tfidf}
    \end{table}

    The terms with the highest average tf-idf scores for the five classes are given in Table \ref{tab:avg_tfidf}.

    \item 
    \begin{figure}
      \centering
      \includegraphics[width=.55\textwidth]{figures/kmeans_2-3-2b}
      \caption{The 0/1 loss as a function of iterations of K-means with $K=5$ (BBC data)}
      \label{fig:kmeans_2-3-2b}
    \end{figure}
    Figure \ref{fig:kmeans_2-3-2b} plots the classification error for this dataset using K-means with $K=5$ and initial means given in the {\tt bbc.centers} file.

    \item 
    \begin{figure}
      \centering
      \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=.95\linewidth]{figures/EM_z1loss_2-3-2c}
        \caption{The 0/1 loss as a function of iterations of EM for GMM}
        \label{fig:EM_z1loss_2-3-2c}
      \end{subfigure}
      \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=.95\linewidth]{figures/EM_likelihood_2-3-2c}
        \caption{The log-likelihood as a function of iterations of EM for GMM}
        \label{fig:EM_likelihood_2-3-2c}
      \end{subfigure}
      \caption{Some results from applying five iterations of EM for GMM to the BBC data.}
    \end{figure}

    Figure \ref{fig:EM_z1loss_2-3-2c} plots the 0/1 loss and Figure \ref{fig:EM_likelihood_2-3-2c} plots the log-likelihood over five iterations of EM applied to the BBC data.

  \end{enumerate}

	{\bf 3. Scaling up K-Means with MapReduce  [35 Points]}
	\\

	The k-means algorithm fits naturally in the MapReduce Framework. Specifically, each iteration of k-means corresponds to one MapReduce cycle: 
	\begin{itemize}
	  \item The map function maps each document with key being the cluster id.
	  \item During the shuffling phase, each documents with the same cluster id will be sent to the same reducer.
	  \item The reduce function reduces on the clusterid and updates the cluster center.
	\end{itemize}

	Because we do not have the ground truth labels for this dataset, the evaluation will be done on the k-means objective function:
	\begin{eqnarray}
	  J(\mathcal{Z},\mu) &=&  \sum_{i=1}^N  \|x^i - \mu_{z^i}\|_2^2 \\
	  (\mathcal{Z}^*, \mu^*) &=& \arg\min_{\mathcal{Z}, \mu} J(\mathcal{Z},\mu)
	\end{eqnarray}
	where $z^i$ is the cluster assignment of example $i$, $\mu_j$ is the center of cluster $j$. 

	To get the value of $J(\mathcal{Z},\mu)$ for each iteration, the reducer with key $j$ will also compute and output the sum of the squares of $L_2$ distance in cluster $j$. At the end of each iteration, you can examine the mean and standard deviation of the clusters. 

	Run k-means with $K = 20$ for 5 iterations. Use the initial cluster centers from ``center0.txt''.
	\begin{enumerate}
	  \item  \textbf{Plot the objective function value $J(\mathcal{Z},\mu)$  versus the number of iterations.}
	  \item  \textbf{For each iteration, report mean (top 10 words in tfidf) of the largest 3 clusters  (by total squared $L_2$ distance). (Use the printClusters function in the starter code.)}
	  % \item  \textbf{Assuming you have 64 machines, without
	  %   actually running experiments, try to estimate the relative runtime
	  %   among $K = 10, 100, 10000$. What is the bottleneck of each
	  %   setting?}
	\end{enumerate}

  \solution

  \begin{enumerate}
    \item 
    \begin{figure}
      \centering
      \includegraphics[width=.75\textwidth]{figures/kmeans_2-4}
      \caption{The value of the objective function $J(\mathcal{Z},\mu)$ as a function of the number of iterations of K-means using Hadoop.} 
      \label{fig:kmeans_2-4}
    \end{figure}

    Figure \ref{fig:kmeans_2-4} plots the value of $J(\mathcal{Z},\mu)$ against the number of iterations.

    \item 

    \begin{table}
      \centering
      \begin{tabular}{|c|c|c|} \hline
          {\bf Cluster 9 (982112)} & {\bf Cluster 12 (22697)} & {\bf Cluster 10 (17885)}\\ \hline
          season      &   church      &   album       \\ \hline
          family    &   college     &   released    \\ \hline
          females   &   general     &   songs       \\ \hline
          high      &   served      &   rock        \\ \hline
          north     &   god         &   musical     \\ \hline
          station   &   published   &   awards      \\ \hline
          located   &   house       &   records     \\ \hline
          club      &   students    &   featured    \\ \hline
          second    &   family      &   concert     \\ \hline
          well      &   member      &   group       \\ \hline
      \end{tabular}
      \caption{Top 10 words in tfidf of the largest three clusters after one iteration of K-means.}
      \label{tab:iteration1}
    \end{table}

    \begin{table}
      \centering
      \begin{tabular}{|c|c|c|} \hline
          {\bf Cluster 9 (98388)} & {\bf Cluster 12 (26008)} & {\bf Cluster 10 (12898)}\\ \hline
          season    &   church      &   album       \\ \hline
          family    &   college     &   released    \\ \hline
          females   &   served      &   songs       \\ \hline
          station   &   general     &   records     \\ \hline
          north     &   published   &   rock        \\ \hline
          high      &   house       &   musical     \\ \hline
          located   &   appointed   &   featured    \\ \hline
          system    &   member      &   awards      \\ \hline
          club      &   students    &   chart       \\ \hline
          group     &   royal       &   concert     \\ \hline
      \end{tabular}
      \caption{Top 10 words in tfidf of the largest three clusters after two iterations of K-means.}
      \label{tab:iteration2}
    \end{table}

    \begin{table}
      \centering
      \begin{tabular}{|c|c|c|} \hline
          {\bf Cluster 9 (97070)} & {\bf Cluster 12 (29055)} & {\bf Cluster 10 (10009)}\\ \hline
          season    &   church      &   album       \\ \hline
          females   &   college     &   released    \\ \hline
          family    &   served      &   songs       \\ \hline
          station   &   appointed   &   records     \\ \hline
          north     &   member      &   rock        \\ \hline
          located   &   house       &   musical     \\ \hline
          high      &   published   &   chart       \\ \hline
          system    &   general     &   featured    \\ \hline
          group     &   students    &   concert     \\ \hline
          well      &   royal       &   bass        \\ \hline
      \end{tabular}
      \caption{Top 10 words in tfidf of the largest three clusters after three iterations of K-means.}
      \label{tab:iteration3}
    \end{table}

    \begin{table}
      \centering
      \begin{tabular}{|c|c|c|} \hline
          {\bf Cluster 9 (95311)} & {\bf Cluster 12 (31232)} & {\bf Cluster 10 (8085)}\\ \hline
          females   &   church      &   album       \\ \hline
          family    &   college     &   released    \\ \hline
          station   &   served      &   songs       \\ \hline
          located   &   member      &   records     \\ \hline
          north     &   appointed   &   rock        \\ \hline
          high      &   house       &   chart       \\ \hline
          season    &   published   &   musical     \\ \hline
          system    &   general     &   featured    \\ \hline
          group     &   party       &   bass        \\ \hline
          well      &   royal       &   track       \\ \hline
      \end{tabular}
      \caption{Top 10 words in tfidf of the largest three clusters after four iterations of K-means.}
      \label{tab:iteration4}
    \end{table}

    \begin{table}
      \centering
      \begin{tabular}{|c|c|c|} \hline
          {\bf Cluster 9 (93304)} & {\bf Cluster 12 (32564)} & {\bf Cluster 10 (6968)}\\ \hline
          females   &   church:     &   album       \\ \hline
          family    &   college     &   released    \\ \hline
          station   &   served      &   songs       \\ \hline
          located   &   member      &   records     \\ \hline
          north     &   appointed   &   rock        \\ \hline
          high      &   house       &   chart       \\ \hline
          system    &   published   &   bass        \\ \hline
          group     &   party       &   track       \\ \hline
          well      &   general     &   featured    \\ \hline
          white     &   royal       &   musical     \\ \hline
      \end{tabular}
      \caption{Top 10 words in tfidf of the largest three clusters after five iterations of K-means.}
      \label{tab:iteration5}
    \end{table}

    Tables \ref{tab:iteration1} through \ref{tab:iteration5} give the top ten words in each of the three clusters with largest in-cluster distances for the five iterations of K-means.
  \end{enumerate}

















































\end{homeworkProblem}

\end{document}