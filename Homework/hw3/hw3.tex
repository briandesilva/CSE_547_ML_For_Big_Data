\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption,subcaption}
\usepackage{subfig}
\usepackage{enumerate}          % For enumerates indexed by letters
\usepackage{bm}                 % For bold letters
\usepackage{algorithm2e}        % For pseudocode
\usepackage{url}                % So texttt wraps instead of creating hbox
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}		% So only referenced equations are numbered

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=7in
\textheight=9.0in
\headsep=0.25in
\linespread{1.1}
\pagestyle{fancy}

\lhead{\hmwkAuthorName}
\chead{\hmwkClass:\ \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}


%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Homework 3}
\newcommand{\hmwkDueDate}{May 19, 2017}
\newcommand{\hmwkClass}{CSE 547}
\newcommand{\hmwkAuthorName}{Brian de Silva}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ }\\
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}


%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pd}[2]{\frac{\partial}{\partial #1} (#2)}

\newcommand{\pdd}[2]{\frac{\partial #1}{\partial #2}}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\vskip 0.2cm \large Solution:\\}}

% Useful commands
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\bbm}{\begin{bmatrix}}
\newcommand{\ebm}{\end{bmatrix}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\dtdx}{\frac{\Delta t}{\Delta x}}
\newcommand{\half}{\frac12}
\newcommand{\norm}[1]{\left\|#1\right\|}


\begin{document}

\maketitle

\pagebreak

\section*{Collaborators}
I collaborated with Emily Dinan on Problem 1.5.



% Problem 1
\section{(Dual) Coordinate Ascent}


In this problem, we will derive the dual coordinate ascent algorithm for the special case of ridge regression. In particular, we seek to solve the problem:
\begin{align}
	\min_w L(w) \textrm{ where } L(w)= \sum_{i=1}^n (w\cdot x_i - y_i)^2 + \lambda \|w\|^2
\end{align}

Recall that the solution is:
\begin{align}
	w^* = (X^\top X+\lambda I)^{-1} X^\top Y
\end{align}
where $X$ be the $n\times d$ matrix whose rows are $x_i$, and $Y$ is an $n$-dim vector.

Recall the argument:
\begin{eqnarray*}
	w^* & = &  (X^\top X+\lambda I)^{-1} X^\top Y\\
	 & = &  X^\top (X X^\top+\lambda I)^{-1} Y\\
	 & := &  \frac{1}{\lambda} X^\top \alpha^*
\end{eqnarray*}
where $\alpha^* = (I+XX^\top/ \lambda )^{-1} Y$.

\begin{enumerate}
	\item (Linear algebra brush up) Prove that the above manipulations are true (without any assumptions on $X$ or $n$ or $d$). This involves proving the second equality in the expression.
\end{enumerate}

As in class, define:
\begin{align}
	G(\alpha_1, \alpha_2, \ldots \alpha_n) = \frac{1}{2} \alpha^\top
	(I+XX^\top/ \lambda ) \alpha - Y^\top \alpha
\end{align}

Now let us consider the consider the coordinate ascent algorithm:
\begin{itemize}
	\item start with $\alpha=0$.
	\item choose coordinate $i$ randomly, and update:
	\begin{align}
	\alpha_i = \argmin_z G(\alpha_1, \ldots \alpha_{i-1}, z, \ldots, \alpha_n)
	\end{align}
	\item repeat the previous step until some termination criterion is reached.
	\item return $w =\frac{1}{\lambda} X^\top \alpha$.
\end{itemize}

\begin{enumerate}
\setcounter{enumi}{1} 
	\item Show that the solution to the inner optimization problem for $\alpha_i$ is:
	\begin{align}
	\alpha_i = \frac{ (y_i - \frac{1}{\lambda}(\sum_{j\neq i} \alpha_j x_j)\cdot x_i )}{1+\|x_i\|^2/\lambda} 
	\end{align}
	\item What is the computational complexity of this update, as it is stated? (Assume that scalar multiplication and scalar addition is order one.)
	\item What is the computational complexity of one stochastic gradient descent update?
\end{enumerate}

Now let us consider the update rule from class:

\begin{itemize}
	\item start with $\alpha=0$, $w=\frac{1}{\lambda} X^\top \alpha=0$.
	\item Choose coordinate $i$ randomly and perform the following update:
	\begin{itemize}
		\item Compute the differences:
		\begin{equation}\label{eq:delta}
			\Delta \alpha_i =\frac{ (y_i -w\cdot x_i )-\alpha_i}{1+\|x_i\|^2/\lambda} 
		\end{equation}
		\item Update the parameters as follows:
		\begin{equation}\label{eq:update}
			\alpha_i \leftarrow \alpha_i+\Delta\alpha_i, \, \quad w \leftarrow w+\frac{1}{\lambda}x_i \cdot \Delta\alpha_i
		\end{equation}
	\end{itemize}
	\item Repeat the previous step until some termination criterion is reached.
	\item return $w =\frac{1}{\lambda} X^\top \alpha$.
\end{itemize}

Now let us examine why this algorithm is more efficient.

\begin{enumerate}
\setcounter{enumi}{4} 
	\item Prove that the above update is valid.
	\item What is the computational complexity of the update, Equation \eqref{eq:update}, in the above implementation?
\end{enumerate}

\solution
\begin{enumerate}
	\item Note that both $XX^\top$ and $X^\top X$ are symmetric positive-semidefinite, so all of their eigenvalues $\lambda_i$ satisfy $\lambda_i\geq0$ (they share the same eigenvalues, with the larger of the two matrices having 0 as a repeated eigenvalue with higher algebraic multiplicity). The eigenvalues of $X^\top X+\lambda$ and $XX^\top + \lambda I$ are therefore $ \lambda_i + \lambda$. If $\lambda>0$ then these eigenvalues are strictly positive, in which case it follows that both matrices are invertible. Hence
	\begin{align}
		\begin{array}{crcl}
			&X^\top Y&=&X^\top Y \\
			\iff & X^\top Y &=& X^\top I Y \\
			\iff & X^\top Y &=& X^\top(XX^\top+\lambda I)(XX^\top+\lambda I)^{-1}Y \\
			\iff & X^\top Y &=& (X^\top X+\lambda I)X^\top(XX^\top+\lambda I)^{-1}Y \\
			\iff & (X^\top X+\lambda I)^{-1}X^\top Y &=& X^\top(XX^\top+\lambda I)^{-1}Y.
		\end{array}
	\end{align}
	This shows that the second equality in the expression is true.
	\item Differentiating $G$ with respect to $\alpha_i$ yields
	\begin{align*}
		\frac{\partial G}{\partial \alpha_i} &= [(I+\tfrac1\lambda XX^\top)\alpha]_i-y_i\\
		&= (I+\tfrac1\lambda XX^\top)_i\cdot \alpha - y_i\\
		&= \sum_{j\neq i}^n\left(\frac{x_i\cdot x_j}{\lambda}\alpha_j\right) + \left(1+\frac{x_i\cdot x_i}{\lambda} \right)\alpha_i-y_i\\
		&= \tfrac1\lambda\left(\sum_{j\neq i}^n \alpha_jx_j\right)\cdot x_i + \left(1+\frac{\norm{x_i}^2}{\lambda} \right)\alpha_i-y_i.
	\end{align*}
	where $(I+\tfrac1\lambda XX^\top)_i$ is the $i$-th row of $I+\tfrac1\lambda XX^\top$. Note that we have used that $(XX^\top)_{ij} = x_i\cdot x_j$. To find the optima we set the above expression equal to zero and solve for $\alpha_i$
	\begin{align}\label{eq:inneroptimization}
		\begin{array}{crcl}
			 &\tfrac1\lambda\left(\sum_{j\neq i}^n \alpha_jx_j\right)\cdot x_i + \left(1+\frac{\norm{x_i}^2}{\lambda} \right)\alpha_i-y_i & = & 0\\
			\implies & \left(1+\frac{\norm{x_i}^2}{\lambda} \right)\alpha_i & = & y_i - \tfrac1\lambda\left(\sum_{j\neq i}^n \alpha_jx_j\right)\cdot x_i \\
			\implies & \alpha_i & = & \frac{y_i - \tfrac1\lambda\left(\sum_{j\neq i}^n \alpha_jx_j\right)\cdot x_i}{1+\norm{x_i}^2/\lambda}.
		\end{array}
	\end{align}
	\item As it is stated the most expensive part of the update is the computation of the sum $\sum_{j\neq i}^n \alpha_jx_j$. For each of the $n-1$ terms one must perform $d$ multiplications, totalling $\mathcal{O}(nd)$ operations. Adding these terms together costs us another $d(n-1)=\mathcal{O}(nd)$ operations. The other computations are relatively cheap compared to this one, so the total complexity is $\mathcal{O}(nd)$.
	\item An update of stochastic gradient descent is of the form
	\begin{align}
		w\leftarrow w + \eta\left(-\lambda w + x^j\left(y^j-\frac{\exp(w_0+w\cdot x^j)}{1+\exp(w_0+w\cdot x^j)} \right)\right)
	\end{align}
	which only costs $\mathcal{O}(d)$ operations (the vector additions, dot products, and multiplication of $x^j$ by a scalar all cost $\mathcal{O}(d)$ operations).

	\item First we observe that
	\begin{align}
		\Delta \alpha_i&=\frac{(y_i-w\cdot x_i)-\alpha_i}{1+\norm{x_i}^2/\lambda}\\
		&=\frac{y_i-\tfrac1\lambda \left(\sum_{j}\alpha_jx_j\right)\cdot x_i-\alpha_i}{1+\norm{x_i}^2/\lambda}\\
		&=\frac{y_i-\tfrac1\lambda \left(\sum_{j\neq i}\alpha_jx_j\right)\cdot x_i-\alpha_i\left(1+\tfrac1\lambda \norm{x_i}^2\right)}{1+\norm{x_i}^2/\lambda}\\
		&= \frac{y_i-\tfrac1\lambda \left(\sum_{j\neq i}\alpha_jx_j\right)\cdot x_i}{1+\norm{x_i}^2/\lambda}-\alpha_i.
	\end{align}
	Hence the update \eqref{eq:update} reduces to
	\begin{equation}
		\alpha_i+\Delta\alpha_i ~=~ \alpha_i + \frac{y_i-\tfrac1\lambda \left(\sum_{j\neq i}\alpha_jx_j\right)\cdot x_i}{1+\norm{x_i}^2/\lambda}-\alpha_i ~=~ \frac{y_i-\tfrac1\lambda \left(\sum_{j\neq i}\alpha_jx_j\right)\cdot x_i}{1+\norm{x_i}^2/\lambda},
	\end{equation}
	which is the solution to the inner optimization problem for $\alpha_i$ as demonstrated in \eqref{eq:inneroptimization}.

	Next we consider the update for $w$. Before updating, $w=\tfrac1\lambda X^\top \alpha$. Then $\alpha$ is updated as in \eqref{eq:update}, which can also be expressed as $\alpha\leftarrow \alpha + e_i\Delta\alpha_i$, where $e_i$ is the vector with 0's in all but the $i$-th entry where it has a 1. Thus the new $w$ should be
	\begin{equation}
		\begin{array}{crl}
		&w&\leftarrow \tfrac1\lambda X^\top(\alpha+e_i\Delta\alpha_i)\\
		\implies&w&\leftarrow \tfrac1\lambda X^\top\alpha + \tfrac1\lambda X^\top e_i\Delta\alpha_i\\
		\implies&w&\leftarrow w + \tfrac1\lambda x_i\cdot\Delta\alpha_i
		\end{array}
	\end{equation}
	which agrees with the update given in \eqref{eq:update}.

	\item The computational complexity of the update in Equation \eqref{eq:update} is of complexity $\mathcal{O}(d)$ since computing $\Delta \alpha_i$ requires two $d$-dimensional dot products, each costing $2d-1=\mathcal{O}(d)$ operations. Updating $w$ also costs $\mathcal{O}(d)$ operations because it requires multiplying a $d$-dimensional vector by a scalar and adding two $d$-dimensional vectors.
\end{enumerate}


% Problem 2
\section{Let's try it out!}
\subsubsection*{Dimension Reduction with PCA}

Project each image onto the top $50$ PCA directions. This reduces the dimension of each image from $784$ to $50$. The reason for doing this is to speed up the computation.

\subsubsection*{Feature generation: Random Fourier Features to approximate the RBF Kernel}
Now we will map each $x$ to a $k$-dimensional feature vector as follows:
\begin{align}
	x \rightarrow (h_1(x), h_2(x), \ldots h_k(x))
\end{align}
We will use $k=N=30,000$.  Each $h_j$ will be of the form:
\begin{align}
	h_j(x) = sin\left(\frac{v_j \cdot x}{\sigma}\right)
\end{align}
To construct all these vectors $v_1, v_2,\ldots v_k$, you will independently sample every coordinate for every vector from a standard normal distribution (with unit variance). Here, you can think of $\sigma$ as a bandwidth parameter.

\subsection{SGD and Averaging}
Let us optimize the square loss. We do this as a surrogate to obtain good classification accuracy (on the test set).

In practice, instead of complete randomization, we often run in \emph{epochs}, where we randomly permute our dataset and then pass through out dataset in this permuted order. Each time we start another epoch, we permute our dataset again.

Let us plot the losses of two quantities. Let us keep around your parameter vector $w_t$ (your weight vector at iteration $t$). Let us also track the average weight vector $\overline{w}_\tau$ over the last epoch, i.e. $\overline{w}_\tau$ is the average parameter vector over the $\tau$-th epoch.  The average parameter is what is suggested to be used by the Polyak-Juditsky averaging method.

Define the total square loss as the \emph{sum} square loss over the $10$ classes (so you do not divide by $10$ here).

Throughout this question you will use a mini-batch size of $1$.

\begin{enumerate}
	\item Specify your parameter choices: your learning rate (or learning rate scheme) and the kernel bandwidth.
	\item You should have one plot showing the both the squared loss after every epoch (starting with your initial squared error). Please label your axes in a readable way.  Plot the loss of both $w_t$ and the average weight vector $\overline{w}_\tau$. You should have both the training and test losses on the same plot (so there should be four curves).
	\item For the classification loss, do the same, except start your plots at the end of the first epoch. Again, there should be four curves (for the training and test loss of both parameters).
	\item  What is your final training squared loss and classification loss for both parameters? What is the total number of mistakes that are made on the training set of your final point? Comment on the performance of the average iterate vs. the last point.
	\item What is your final test squared loss and classification loss for both parameters? What is the total number of mistakes that are made on the test set of your final point? Comment on the performance of the average iterate vs. the last point.
\end{enumerate}

\solution

\begin{enumerate}
	\item For this problem I used no regularization, a learning rate of $10^{-5} / (2\sqrt{t+1})$, where $t$ is the epoch, and set all initial weights to 0. My kernel bandwidth was half the approximate mean distance between points in the dataset. In order to approximate this distance I randomly sampled 100 pairs of points from the training data and took the mean of their Euclidean distances from one another. This parameter was on the order of $10^3$. To obtain the plots below I ran SGD for 10 epochs. The code took about two hours to run (including the time it took to compute the square and classification loss each epoch).

	\item
	\begin{figure}
		\centering
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[width=.95\linewidth]{figures/SGD_squareLoss}
			\caption{The square loss of SGD on the training and test sets as a function of the epoch}
			\label{fig:SGD_squareLoss_overview}
		\end{subfigure}
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{figures/SGD_squareLoss_zoomed}
			\caption{The square loss of SGD on the training and test sets as a function of the epoch (starting after the first epoch)}
			\label{fig:SGD_squareLoss_zoomed}
		\end{subfigure}
		\caption{Two views of the square loss of SGD on the training and test sets after each epoch.}
		\label{fig:SGD_squareloss}
	\end{figure}
	Figure \ref{fig:SGD_squareLoss_overview} shows the square loss after each epoch on the training and test sets for both $w_t$ and the average weight vector $\overline w_\tau$. Figure \ref{fig:SGD_squareLoss_zoomed} shows the same square loss, but omits the initial loss.

	\item
	\begin{figure}
		\centering
		\includegraphics[width=.75\textwidth]{figures/SGD_01Loss}
		\caption{The classification error of SGD on the training and test sets as a function of the epoch} 
		\label{fig:figures/SGD_01Loss}
	\end{figure}
	Figure \ref{fig:figures/SGD_01Loss} gives the classification loss as a function of iterations for $w_t$ and $\overline w_\tau$ on the training and test sets.

	\item
	\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|c|c|}\hline
		     & {\bf Square loss} & {\bf Classification loss} & {\bf Total mistakes}  \\ \hline
		     $w_t$            & 0.056793	& 0.009250	 & 555	\\ \hline
		     $\overline w_\tau$  & 0.056665	& 0.009433	 & 566	\\ \hline
		\end{tabular}
		\caption{Final losses of SGD on the training set}
		\label{tab:sgd_final_loss_train}
	\end{table}

	The final squared and classification losses on the training set are summarized in Table \ref{tab:sgd_final_loss_train}. The average iterate and the final weights produced by SGD give similar performance on the training set with the final weights doing marginally better.

	\item
	\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|c|c|}\hline
		     & {\bf Square loss} & {\bf Classification loss} & {\bf Total mistakes}  \\ \hline
		     $w_t$            & 0.071672	& 0.017400	 & 174	\\ \hline
		     $\overline w_\tau$  & 0.071346	& 0.016500	 & 165	\\ \hline
		\end{tabular}
		\caption{Final losses of SGD on the test set}
		\label{tab:sgd_final_loss_test}
	\end{table}

	The final squared and classification losses on the test set are summarized in Table \ref{tab:sgd_final_loss_test}. Again the performance of the averaged weights and the final weights are similar. On the test set the average weights do a bit better than the final weights.
\end{enumerate}



\subsection{SDCA}
Now let us try out our (dual) coordinate ascent method. Again, instead of complete randomization, we often run in \emph{epochs}, where we randomly permute our dataset and then pass through out dataset in this permuted order. Each time we start another epoch, we permute our dataset again.

Choose a regularization parameter. Do this appropriately so you obtain a good test accuracy.

\begin{enumerate}
	\item Specify your regularization parameter. Are there any other parameter choices?
	\item  Now plot the dual loss $G$. Your plot should show the $G$-loss after every epoch (starting with your initial squared error). Please label your axes in a readable way.
	\item Does it make sense to plot the $G$-loss on the test set? Why not?
	\item  Now plot the squared loss after every epoch (starting with your initial squared error). Please label your axes in a readable way.  You should have both the training and test losses on the same plot (so there should be two curves).
	\item  For the classification loss, do the same, except start your plots at the end of the first epoch. Again, there should be two curves (for the training and test loss).
	\item   What is your best training squared loss and classification loss? What is the total number of mistakes that are made on the training set of your final point?
	\item   What is your best test squared loss and classification loss? What is the total number of mistakes that are made on the test set of your final point?
	\item Compare the speed and accuracy to what you obtained with SGD?
\end{enumerate}

\solution

\begin{enumerate}
	\item I chose $\lambda=1.0$ as my regularization parameter. I ran the method for one epoch using a variety of values of $\lambda$ and found this one to give the best 0/1 loss on the test set. The only other parameters to set were hyperparameters such as the number of epochs to run.

	\item
	\begin{figure}
		\centering
		\includegraphics[width=.75\textwidth]{figures/SDCA_gloss_b1}
		\caption{G-loss on training set using SDCA with batch size $b=1$} 
		\label{fig:SDCA_gloss_b1}
	\end{figure}
	Figure \ref{fig:SDCA_gloss_b1} shows the G-loss after each of the 15 epochs.

	\item It does not make sense to plot the G-loss on the test set because the G-loss is meant to give information about how close our current $\alpha$ is to the true dual coordinates, $\alpha^*$. But $\alpha^*$ is associated with the solution to the least squares problem involving only the \emph{training set}. Furthermore, $\alpha$ does not even have the right number of entries for us to define a G-loss on the test set (unless the training and test sets were the same size). Each entry of $\alpha_i$ corresponds to an example in the training set.

	\item
	\begin{figure}
		\centering
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{figures/SDCA_squareLoss_b1}
			\caption{The square loss for SDCA using batch size $b=1$}
		\end{subfigure}
		\begin{subfigure}{0.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{figures/SDCA_squareLoss_zoomed_b1}
			\caption{The square loss after 1 epoch for SDCA using batch size $b=1$}
		\end{subfigure}
		\caption{Two views of the square loss for SDCA using batch size $b=1$} 
		\label{fig:SDCA_squareLoss_b1}
	\end{figure}
	Figure \ref{fig:SDCA_squareLoss_b1} shows two views of the square loss of SDCA on the training and test data. One subfigure omits the initial square error.

	\item
	\begin{figure}
		\centering
		\includegraphics[width=.75\textwidth]{figures/SDCA_z1Loss_b1}
		\caption{The classification error of SDCA on the training and test sets as a function of the epoch} 
		\label{fig:SDCA_z1Loss_b1}
	\end{figure}
	Figure \ref{fig:SDCA_z1Loss_b1} plots the classification (0/1) loss on the training and test sets as a function of epochs of SDCA.

	\newpage

	\item
	\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|c|c|}\hline
		     & {\bf Square loss} & {\bf Classification loss} & {\bf Total mistakes}  \\ \hline
		     Training	& 0.045556	& 0.001267	 & 76	\\ \hline
		     Test    	& 0.093776	& 0.014500	 & 145	\\ \hline
		\end{tabular}
		\caption{Final losses of SDCA on the training and test sets using batch size $b=1$}
		\label{tab:SDCA_final_loss_b1}
	\end{table}
	Table \ref{tab:SDCA_final_loss_b1} summarizes the best square and classification losses on the training and test sets, as well as the total number of mistakes made on each.

	\item See Table \ref{tab:SDCA_final_loss_b1}.

	\item SDCA is able to produce a solution with slightly better accuracy than SGD on the test set (and much better accuracy on the training set) in roughly the same amount of time (actually almost the exact same amount of time--SDCA only took about 17 seconds longer).
\end{enumerate}


\subsection{Mini-batching (in the dual) and parallelization!}
Now, let us understand parallelization issues with mini-batching. 

Here instead of updating one coordinate at a time, we will update multiple coordinates. In particular, we will choose a batch of  $b$ coordinates and compute the $\Delta_i$'s of these coordinates as specified in Equation \eqref{eq:delta}.
 
Let us choose a batch size $b=100$.

\begin{enumerate}
	\item One possibility is to just update the $\alpha_i$'s as specified in Equation \eqref{eq:update}. Is this ok?
	\item Argue that the following update never increases the value of $G$:
	\begin{align}
		\alpha_i \leftarrow \alpha_i+\frac{1}{b}\Delta\alpha_i, \, \quad w \leftarrow
		w+\frac{1}{b}\frac{1}{\lambda}x_i \cdot \Delta\alpha_i
	\end{align}
	where the update is performed on all of the chosen coordinates in the mini-batch.
	\item  Now use this algorithm with $b=100$. And provide the same training and test loss plots, for the square loss and classification loss as before. (Again, use a permuted order rather than complete randomization). Plot this with your earlier curves for $b=1$ and have the $x$ axis be the total number of points used. Note that $b$ steps on your $x$-axis will correspond to one update.
	\item In terms of the \emph{total number of data points touched}, does this perform notably better or worse than what you had before, with the $b=1$?
\end{enumerate}

\solution

\begin{enumerate}
	\item No. Consider the extreme case when all $b$ $x_i$'s we select are the same (or are collinear). Since we compute all the $\Delta\alpha_i$'s without updating $w$, they could all end up having roughly the same value, say $\Delta \alpha$. When we go to update $w$, instead of modifying it how we would have before, our update is approximately
	\begin{equation}
		w\leftarrow w + \tfrac{b}{\lambda}x_i\cdot\Delta\alpha.
	\end{equation}
	This pushes $w$ a factor of $b$ too far in the direction $x_i$ (remember that all the $b$ samples are assumed to be roughly $x_i$).
	\item Let $I$ be the set of indices which are to be updated in a given batch so that $|I|=b$. Note that for the same reasons \eqref{eq:update} was shown to be valid, updating any one component of $\alpha$ using the rule $\alpha_i\leftarrow \alpha_i+\Delta\alpha_i$ results in a decrease in $G$. Where we can run into problems is when we update many such components of $\alpha$ in this way before updating $w$. Observe that we can express all the updates to $\alpha$ in one batch as
	\begin{equation}
		\alpha\leftarrow \alpha+\tfrac1b\sum_{i\in I}\Delta\alpha_ie_i,
	\end{equation}
	where $e_i$ is again the $i$-th standard basis vector. Hence the new value of $G$ after such a series of updates is
	\begin{equation}
		G\left(\alpha+\tfrac1b\sum_{i\in I}\Delta\alpha_ie_i\right).
	\end{equation}
	Since $|I|=b$ this can be rewritten as
	\begin{equation}
		G\left(\tfrac1b\sum_{i\in I}\left(\alpha+\Delta\alpha_ie_i\right)\right),
	\end{equation}
	i.e. as a convex combination of points. Note that $\alpha+\Delta\alpha_ie_i$ is equivalent to the update for $\alpha_i$ given in \eqref{eq:update}. Then, as $G$ is convex, we have
	\begin{align}
		G\left(\alpha+\tfrac1b\sum_{i\in I}\Delta\alpha_ie_i\right) &=G\left(\tfrac1b\sum_{i\in I}\left(\alpha+\Delta\alpha_ie_i\right)\right)\\
		&\leq \tfrac1b\sum_{i\in I}G(\alpha+\Delta\alpha_ie_i)\\
		&\leq \tfrac1b\sum_{i\in I}G(\alpha)\\
		&=G(\alpha).
	\end{align}
	Hence the proposed update never increases the value of $G$. The update for $w$ simply changes $w$ to reflect the updates made in the $b$ components of $\alpha$.

	\item ***Plots***

	\item ***Discussion***
\end{enumerate}


Now let us improve upon this algorithm. Consider the update rule with parameter $\gamma$:
\begin{align}
	\alpha_i \leftarrow \alpha_i+\gamma\Delta\alpha_i, \, \quad w \leftarrow
	w+\frac{\gamma}{\lambda}x_i \cdot \Delta\alpha_i
\end{align}
where the update is performed on all of the chosen coordinates in the mini-batch.


\begin{enumerate}
	\item Give an argument as to why choosing $\gamma=1/b$ is pessimistic. (Either give a concise argument or give an example.)
	\item Now empirically (on our $b=100$ experiments) search a little to find a good choice for gamma. What value of $\gamma$ did you choose? What value  of $\gamma$ do things diverge?
	\item  Now use this choice to redo your experiments. Provide the same training and test loss plots, for the square loss and classification loss as before. (Again, use a permuted order rather than complete randomization).
	\item In terms of the \emph{total number of data points touched}, does this perform notably better than what you had before, for both SGD and for the $\gamma=1/b$ case? What have noticed about your total runtime?
\end{enumerate}

\solution

\begin{enumerate}
	\item Using the example from before where all of the samples in our batch are identical, i.e. they are all $(x_1,y_1)$, we saw that if we did not divide our batch updates by a factor of $b$, each term would contribute the same amount in the same direction to $w$ when the update $w\leftarrow w+\tfrac\gamma\lambda x_i\cdot\Delta\alpha_i$ was applied. $w$ would be pushed $b$ times too far in the direction of $x_i=x_1$. In this extreme we are forced to choose $\gamma=\tfrac1b$ to counteract the additive effect of all these updates. Of course the drawback of doing this is that we can only update each $\alpha_i$ by adding $\tfrac1b\Delta\alpha_i$ rather than adding the amount suggested by solving the optimization problem for a single coordinate, $\Delta\alpha_i$. In most circumstances, however, the examples will not be identical or even collinear, so the effects of the different updates will not accumulate in one direction when updating $w$. This should allow us to better optimize $G$ by choosing a larger value of $\gamma$, giving us better approximate solutions to the $b$ single-coordinate optimization problems for the $\alpha_i$'s.

	\item ***Find a good value of $\gamma$***

	\item ***Plots for this value of $\gamma$***

	\item ***Performance discussion***
\end{enumerate}

\end{document}